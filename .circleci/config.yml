version: 2

variables:

  # default settings for all steps
  defaults: &defaults
    docker:
      - image: continuumio/miniconda3

  # --------------------------------------------------------------------------
  # The caching dramatically speeds up testing time, because we can do the
  # time-consuming step of conda environment creation once and then use that
  # for subsequent steps. It works like this:
  #
  # The `initial-setup` job (defined in workflows below) saves the cache when
  # it's done. Here the cache is the miniconda directory. Later jobs
  # (chipseq-step, rnaseq-step, etc) restore that cache to dramatically speed
  # up testing time.
  #
  # The cache key is set to only re-make the cache when the relevant files
  # change. There's also a `v1-` prefix. This should be changed once in a while
  # just to re-trigger an environment rebuild (say, when some critical updates
  # hit bioconda).
  #
  # See https://circleci.com/docs/2.0/caching for details.

  save_cache: &save_cache
    save_cache:
      key: v0-{{ checksum "env.yml" }}-{{ checksum "env-r.yml" }}
      paths:
        - /opt/conda

        # this file is created by sra-tools upon installation by conda, and so
        # needs to be included in the cache otherwise fastq-dump thinks it's
        # mis-configured.
        - /root/.ncbi/user-settings.mkfg

  restore_cache: &restore_cache
    restore_cache:
      keys:
        - v0-{{ checksum "env.yml" }}-{{ checksum "env-r.yml" }}

  # --------------------------------------------------------------------------
  # The path needs to be set each time; in jobs below this will be called as
  # one of the first steps in each job.
  set-path: &set-path
      run:
        name: Set path
        command: |
          # x11-utils required to avoid R::png() segfaulting
          apt update && apt install -y locales-all locales x11-utils
          echo 'export DEPLOY=/tmp/lcdb-wf-test' >> $BASH_ENV
          echo 'export LC_ALL=en_US.utf8' >> $BASH_ENV
          echo 'export LANG=en_US.utf8' >> $BASH_ENV
          source $BASH_ENV
  # --------------------------------------------------------------------------
  # Set up conda if the environments do not already exist
  setup: &setup
    run:
      name: Setup conda
      command: |
        set -e
        apt install -y locales-all locales
        export LC_ALL=en_US.utf8
        export LANG=en_US.utf8
        # We only do the installation if the conda environment does not already
        # exist.
        if ! conda env list | grep -q "lcdb-wf-test"; then
            echo "Setting up conda..."
            conda config --system --add channels defaults
            conda config --system --add channels bioconda
            conda config --system --add channels conda-forge
            conda install mamba -y
            mamba env create -n lcdb-wf-test --file env.yml
            mamba env create -n lcdb-wf-test-r --file env-r.yml
        fi
        # conda env export -n lcdb-wf-test > env.yaml
        # conda env export -n lcdb-wf-test-r > env-r.yaml

  # --------------------------------------------------------------------------
  # Deploy into a new directory and get the test data.
  get-data: &get-data
    run:
      name: Download example data
      command: |
        source activate lcdb-wf-test

        # rsync is required for the deployment process
        apt install -y rsync

        # Deploy to the new directory, so we are testing the real-world case of post-deployment.
        # Note that $DEPLOY is set in the "set-paths" step configured above.
        python deploy.py --flavor full --dest $DEPLOY

        # Separately copy over some test-specific files
        cp workflows/chipseq/run_test.sh $DEPLOY/workflows/chipseq
        cp workflows/rnaseq/run_test.sh $DEPLOY/workflows/rnaseq
        cp workflows/rnaseq/run_downstream_test.sh $DEPLOY/workflows/rnaseq
        cp workflows/colocalization/run_test.sh $DEPLOY/workflows/references
        cp workflows/colocalization/run_test.sh $DEPLOY/workflows/colocalization
        mkdir $DEPLOY/ci
        cp ci/get-data.py $DEPLOY/ci

        # the ./run_test.sh scripts run this
        cp ci/preprocessor.py $DEPLOY/ci

        # download example data
        cd $DEPLOY
        python ci/get-data.py

  # --------------------------------------------------------------------------
  # Run the doctests across the included modules
  pytest-step: &pytest-step
    run:
      name: Run pytest suite
      command: |
        source activate lcdb-wf-test
        pytest --doctest-modules lib

        # find all URLs in reference configs and make sure they exist
        python -c 'from lib.common import check_all_urls_found; check_all_urls_found()'

  # --------------------------------------------------------------------------
  # Other downstream jobs depend on this so that we can catch trivial errors
  # quickly
  rnaseq-dryrun: &rnaseq-dryrun
      run:
        name: dry run
        command: |
          cd $DEPLOY/workflows/rnaseq
          source activate lcdb-wf-test
          ./run_test.sh --use-conda -n
  # --------------------------------------------------------------------------
  # Standard chipseq workflow
  chipseq-step: &chipseq-step
      run:
        name: chipseq workflow
        command: |
          cd $DEPLOY/workflows/chipseq
          source activate lcdb-wf-test
          ./run_test.sh --use-conda -j2 -k -p -r
          python chipseq_trackhub.py config/config.yaml config/hub_config.yaml
  # --------------------------------------------------------------------------
  # Previous versions had an error where chipseq peaks needed to be defined for
  # every caller. This does a (relatively) quick test to only run a single
  # sample through a single peak-caller to prevent regression of that error
  chipseq-misc-step: &chipseq-misc-step
      run:
        name: chipseq misc
        command: |
          ORIG=$(pwd)
          cd $DEPLOY/workflows/chipseq
          source activate lcdb-wf-test
          ./run_test.sh --use-conda -j2 -k -p -r \
            --configfile $ORIG/test/test_configs/test_chipseq_regression.yaml \
            --config sampletable=$ORIG/test/test_configs/chipseq_one_run.tsv \
            merged_bigwigs="{}" \
            --until bed_to_bigbed

          # Piggy-backing on that test, here we check to make sure it's OK to
          # omit peak-calling config. We just do a dry-run.
          ./run_test.sh -n \
            --configfile $ORIG/test/test_configs/test_chipseq_no_peaks.yaml \
            --config sampletable=$ORIG/test/test_configs/chipseq_one_run.tsv \
            merged_bigwigs="{}" \
            --until bed_to_bigbed

  # --------------------------------------------------------------------------
  # Standard references workflow.
  references-step: &references-step
      run:
        name: references workflow
        command: |
          cd $DEPLOY/workflows/references
          source activate lcdb-wf-test
          ./run_test.sh  --use-conda -j2 -k -p -r --configfile config/config.yaml
  # --------------------------------------------------------------------------
  # Standard RNA-seq workflow
  rnaseq-step: &rnaseq-step
      run:
        name: rnaseq workflow
        command: |
          cd $DEPLOY/workflows/rnaseq
          source activate lcdb-wf-test
          ./run_test.sh --use-conda -j2 -k -p -r
          python rnaseq_trackhub.py config/config.yaml config/hub_config.yaml

          # Starting in v1.6, we separate out the non-R from the
          # R environments. So we need to test the rnaseq.Rmd separately
          # outside the context of the Snakefile.
          source activate lcdb-wf-test-r

          # This script runs the preprocessor on the Rmd files and stores them
          # in a new download-test directory (see the comments in the script
          # for details)
          bash run_downstream_test.sh

          # bundle up the entire directory to be used as an artifact
          tar -zcf downstream.tar.gz downstream-test/

  # --------------------------------------------------------------------------
  # Various tests on RNA-seq workflow that don't warrant the overhead of a new
  # cache load
  rnaseq-misc-step: &rnaseq-misc-step
      run:
        name: misc RNA-seq tests
        command: |
          ORIG=$(pwd)
          cd $DEPLOY
          cp -r workflows/rnaseq workflows/rnaseq-misc-test

          # make a copy of the data so we can get a fresh copy each time below
          cp -r workflows/rnaseq/data /tmp/data
          cd workflows/rnaseq-misc-test
          source activate lcdb-wf-test

          # SRA, PE. Note that there's no starting data directory and that we
          # only go to the cutadapt step.
          rm -r data
          ./run_test.sh -j 1 --use-conda -k -p -r --until cutadapt \
            --configfile $ORIG/test/test_configs/test_rnaseq.yaml \
            --config sampletable=$ORIG/test/test_configs/test_sra_sampletable.tsv

          # Same as above, but SE
          rm -r data
          ./run_test.sh -j 1 --use-conda -k -p -r --until cutadapt \
            --configfile $ORIG/test/test_configs/test_rnaseq.yaml \
            --config sampletable=$ORIG/test/test_configs/test_sra_sampletable_SE_only.tsv

          # test strandedness, PE
          rm -r data && cp -r /tmp/data data
          ./run_test.sh -j 2 --use-conda -k -p -r --until strand_check \
            --configfile $ORIG/test/test_configs/test_rnaseq.yaml \
            --config sampletable=$ORIG/test/test_configs/test_pe_sampletable.tsv \
            && cat strandedness.tsv

          # test strandedness, SE
          rm -r data && cp -r /tmp/data data
          ./run_test.sh -j 2 --use-conda -k -p -r --until strand_check \
            --configfile $ORIG/test/test_configs/override.yaml \
            --config sampletable=$ORIG/test/test_configs/two_samples.tsv \
            && cat strandedness.tsv

          # test STAR 2-pass alignment
          rm -r data && cp -r /tmp/data data
          ./run_test.sh -j 2 --use-conda -k -p -r \
            --forcerun star_pass1 \
            --configfile \
              $ORIG/test/test_configs/test_rnaseq_config.yaml \
              $ORIG/test/test_configs/star_override_2pass.yaml \
            --config sampletable=$ORIG/test/test_configs/two_samples.tsv \
            --until star_pass2

          # test STAR 1-pass alignment
          rm -r data && cp -r /tmp/data data
          ./run_test.sh -j 2 --use-conda -k -p -r \
            --forcerun star \
            --configfile \
              $ORIG/test/test_configs/test_rnaseq_config.yaml \
              $ORIG/test/test_configs/star_override_1pass.yaml \
            --config sampletable=$ORIG/test/test_configs/two_samples.tsv \
            --until star

          # test PE reads
          rm -r data && cp -r /tmp/data data
          ./run_test.sh -j 2 --use-conda -k -p -r --until multiqc \
            --configfile \
              $ORIG/test/test_configs/test_rnaseq_config.yaml \
            --config sampletable=$ORIG/test/test_configs/test_pe_sampletable.tsv

  # --------------------------------------------------------------------------
  # Standard colocalization workflow
  colocalization-step: &colocalization-step
      run:
        name: colocalization workflow
        command: |
          cd $DEPLOY/workflows/colocalization
          source activate lcdb-wf-test
          ./run_test.sh -j 1 --use-conda -j2 -k -p -r

# --------------------------------------------------------------------------
# Syntax note: All of the steps above, with their "&step-name" labels, can be
# referred to by a corresponding "*step-name" below. The "<<: *defaults"
# construct is a similar mechanism (YAML anchor) but extends rather than just
# referencing.  https://blog.daemonl.com/2016/02/yaml.html has a nice
# description of the difference.
#
# Here we define jobs as multiple steps by composing what we have defined
# above. These will show up as nodes in the CircleCI workflow DAG.
jobs:

  # This is the only job in which the cache is created. It also blocks all
  # other jobs until it's complete (it is a dependency of all other jobs,
  # configured in the workflows in the last section of this config).
  initial-setup:
    <<: *defaults
    steps:
      - checkout

      # Check the hashes of requirements files. If they match a cache, load it.
      # The cache is set up to be the entire miniconda installation, so that
      # includes the lcdb-wf-test environment.
      #
      # If the hashes do not match the cache, then nothing is loaded and we do
      # not have the lcdb-wf-test environment.
      - *restore_cache

      # This step checks to see if lcdb-wf-test environment exists. Since the
      # only way it could get there at this point is by loading the cache, if
      # it's present then do nothing. Otherwise re-build the environments
      - *setup

      # When done, save the entire miniconda directory to the cache. Since
      # nothing else will run until this job is complete, we ensure that
      # subsequent jobs can load the latest cache without needing to rebuild it
      # themselves.
      - *save_cache

      - store_artifacts:
          path: /tmp/lcdb-wf-test/env.yaml
      - store_artifacts:
          path: /tmp/lcdb-wf-test/env-r.yaml
  pytest:
    <<: *defaults
    resource_class: small
    steps:
      - checkout
      - *restore_cache
      - *setup
      - *pytest-step

  chipseq:
    <<: *defaults
    steps:
      - checkout
      - *restore_cache
      - *set-path
      - *get-data
      - *chipseq-step
      - store_artifacts:
          path: /tmp/lcdb-wf-test/workflows/chipseq/data/chipseq_aggregation/multiqc.html

  chipseq-misc:
    <<: *defaults
    steps:
      - checkout
      - *restore_cache
      - *set-path
      - *get-data
      - *chipseq-misc-step

  rnaseq:
    <<: *defaults
    steps:
      - checkout
      - *restore_cache
      - *set-path
      - *get-data
      - *rnaseq-step
      - store_artifacts:
          path: /tmp/lcdb-wf-test/workflows/rnaseq/downstream.tar.gz
      - store_artifacts:
          path: /tmp/lcdb-wf-test/workflows/rnaseq/downstream/rnaseq.html
      - store_artifacts:
          path: /tmp/lcdb-wf-test/workflows/rnaseq/data/rnaseq_aggregation/multiqc.html

  rnaseq-misc:
    <<: *defaults
    steps:
      - checkout
      - *restore_cache
      - *set-path
      - *get-data
      - *rnaseq-misc-step

  colocalization:
    <<: *defaults
    steps:
      - checkout
      - *restore_cache
      - *set-path
      - *get-data
      - *colocalization-step

  references:
    <<: *defaults
    steps:
      - checkout
      - *restore_cache
      - *set-path
      - *get-data
      - *references-step

  build-docs:
    <<: *defaults
    resource_class: small
    steps:
      - checkout
      - *restore_cache
      - *set-path
      - run:
          name: Install sphinx
          command: source activate lcdb-wf-test && conda install -y sphinx make yaml
      - run:
          name: OK for unknown github host
          command: mkdir -p ~/.ssh/ && echo -e "Host github.com\n\tStrictHostKeyChecking no\n" > ~/.ssh/config
      - add_ssh_keys:
          fingerprints:
            - 2d:0c:b4:27:44:cf:f4:50:cc:14:a4:2b:c2:3c:09:06
      - run:
          name: Build and upload docs
          command: source activate lcdb-wf-test && ci/build-docs.sh
      - store_artifacts:
          path: /tmp/docs.tar.gz

#   report-env:
#     <<: *defaults
#     resource_class: small
#     steps:
#       - checkout
#       - *restore_cache
#       - *set-path
#       - run:
#           name: Report environment
#           command: |
#             conda env export -n lcdb-wf-test > /tmp/env.yaml
#             cat /tmp/env.yaml
#             conda env export -n lcdb-wf-test-r > /tmp/env-r.yaml
#       - store_artifacts:
#           path: /tmp/env.yaml
#       - store_artifacts:
#           path: /tmp/env-r.yaml

# ----------------------------------------------------------------------------
# This section configures the dependencies across jobs.
workflows:
  version: 2
  test-suite:
    jobs:
      - initial-setup
      - pytest:
         requires:
           - initial-setup
      - chipseq:
         requires:
           - initial-setup
           - pytest
      - chipseq-misc:
         requires:
           - initial-setup
           - pytest
      - rnaseq:
          requires:
            - initial-setup
            - pytest
      - rnaseq-misc:
          requires:
            - initial-setup
            - pytest
      - references:
          requires:
            - initial-setup
            - pytest
      - colocalization:
          requires:
            - initial-setup
            - pytest
      - build-docs:
          requires:
            - initial-setup
      # - report-env:
      #     requires:
      #       - rnaseq
      #       - rnaseq-misc
      #       - chipseq
      #       - chipseq-misc
      #       - references
      #       - colocalization
