import sys
import os
import yaml
import pandas as pd

sys.path.insert(0, os.path.dirname(workflow.snakefile) + "/../..")
from lib import utils
from lib import chipseq


configfile: "config/config.yaml"


include: "../references/Snakefile"


REFERENCES = config.get("reference_dir", "../../references")
sampletable = pd.read_table(config["sampletable"], sep="\t", comment="#")
sampletable = sampletable.set_index(sampletable.columns[0], drop=False)
is_paired = utils.detect_layout(sampletable) == "PE"
n = ["1", "2"] if is_paired else ["1"]
SAMPLES = sampletable.iloc[:, 0].values
patterns = yaml.safe_load(open("config/chipseq_patterns.yaml"))["patterns_by_sample"]
peaks = chipseq.add_bams_to_peak_calling(config)


wildcard_constraints:
    n="[1,2]",
    sample="|".join(SAMPLES),


localrules:
    symlinks,
    symlink_targets,


rule targets:
    input:
        patterns["multiqc"],
        expand(patterns["bigwig"], label=sampletable.label),
        [v["bed"] for k, v in peaks.items()],


if utils.detect_sra(sampletable):
    sampletable['orig_filename'] = expand(
        'original_data/sra_samples/{sample}/{sample}_R{n}.fastq.gz', sample=SAMPLES, n=1)

    if is_paired:
        sampletable['orig_filename_R2'] = expand(
            'original_data/sra_samples/{sample}/{sample}_R{n}.fastq.gz', sample=SAMPLES, n=2)

    rule fastq_dump:
        output:
            fastq=expand('original_data/sra_samples/{sample}/{sample}_R{n}.fastq.gz', n=n, allow_missing=True)
        log:
            'original_data/sra_samples/{sample}/{sample}.fastq.gz.log'
        params:
            is_paired=is_paired,
            # extra="-X 100000",  # [enable for test]
        resources:
            mem="1g",
            disk="1g",
            runtime="2h",
        run:
            srr = sampletable.loc[wildcards.sample, "Run"]
            extra = params.get("extra", "")
            if is_paired:
                shell("fastq-dump {srr} --gzip --split-files {extra} &> {log}")
                shell("mv {srr}_1.fastq.gz {output[0]}")
                shell("mv {srr}_2.fastq.gz {output[1]}")
            else:
                shell("fastq-dump {srr} -Z {extra} 2> {log} | gzip -c > {output[0]}.tmp")
                shell("mv {output[0]}.tmp {output[0]}")


rule symlinks:
    input:
        lambda wc: (
            sampletable.loc[wc.sample, ["orig_filename", "orig_filename_R2"]]
            if is_paired
            else sampletable.loc[wc.sample, ["orig_filename"]]
        ),
    output:
        expand(patterns["fastq"], n=n, allow_missing=True),
    threads: 1
    resources:
        mem="1g",
        runtime="10m",
    run:
        assert len(output) == len(input), (input, output)
        for src, linkname in zip(input, output):
            utils.make_relative_symlink(src, linkname)


rule symlink_targets:
    input:
        expand(
            "data/rnaseq_samples/{sample}/{sample}_R{n}.fastq.gz", sample=SAMPLES, n=n
        ),


rule cutadapt:
    input:
        fastq=expand(patterns["fastq"], n=n, allow_missing=True),
    output:
        fastq=expand(patterns["cutadapt"], n=n, allow_missing=True),
    log:
        "data/chipseq_samples/{sample}/{sample}_cutadapt.fastq.gz.log",
    threads: 6
    resources:
        mem="2g",
        runtime="2h",
    params:
        extra=(
            (
                "--nextseq-trim 20 "
                "--overlap 6 "
                "--minimum-length 25 "
                "-a AGATCGGAAGAGCACACGTCTGAACTCCAGTCA "
            )
            + "-A AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT "
            if is_paired
            else ""
        ),
    run:
        if is_paired:
            shell(
                "cutadapt "
                "-o {output[0]} "
                "-p {output[1]} "
                "-j {threads} "
                "{params.extra} "
                "{input.fastq[0]} "
                "{input.fastq[1]} "
                "&> {log}"
            )
        else:
            shell(
                "cutadapt "
                "-o {output[0]} "
                "-j {threads} "
                "{params.extra} "
                "{input.fastq[0]} "
                "&> {log}"
            )


rule fastqc:
    input:
        "{sample_dir}/{sample}/{sample}{suffix}",
    threads: 1
    output:
        html="{sample_dir}/{sample}/fastqc/{sample}{suffix}_fastqc.html",
        zip="{sample_dir}/{sample}/fastqc/{sample}{suffix}_fastqc.zip",
    resources:
        mem="8g",
        runtime="2h",
    log:
        "{sample_dir}/{sample}/fastqc/{sample}{suffix}_fastqc.log",
    run:
        outdir = os.path.dirname(output.html) or "."
        shell(
            "fastqc "
            "--noextract "
            "--quiet "
            "--outdir {outdir} "
            "{input} "
            "&> {log} "
        )
        outfile = os.path.basename(input[0])
        for s in [".fastq", ".fq", ".gz", ".bam"]:
            outfile = outfile.replace(s, "")
        out_zip = os.path.join(outdir, outfile + "_fastqc.zip")
        if not os.path.abspath(out_zip) == os.path.abspath(output.zip):
            shell("mv {out_zip} {output.zip}")
        out_html = os.path.join(outdir, outfile + "_fastqc.html")
        if not os.path.abspath(out_html) == os.path.abspath(output.html):
            shell("mv {out_html} {output.html}")


rule bowtie2:
    input:
        fastq=expand(patterns["cutadapt"], n=n, allow_missing=True),
        index=multiext(
            f"{REFERENCES}/bowtie2/genome",
            ".1.bt2",
            ".2.bt2",
            ".3.bt2",
            ".4.bt2",
            ".rev.1.bt2",
            ".rev.2.bt2",
            ".fa",
        ),
    output:
        bam=temporary(patterns["bam"]),
    log:
        patterns["bam"] + ".log",
    threads: 16
    resources:
        mem="32g",
        runtime="2h",
    params:
        extra="",
    run:
        prefix = os.path.commonprefix(input.index).rstrip(".")
        sam = output.bam.replace(".bam", ".sam")
        fastqs = (
            f"-1 {input.fastq[0]} -2 {input.fastq[1]}"
            if is_paired
            else f"-U {input.fastq}"
        )
        shell(
            "bowtie2 "
            "-x {prefix} "
            "{fastqs} "
            "--no-unal "
            "--threads {threads} "
            "-S {sam} "
            "{params.extra} "
            "> {log} 2>&1"
        )

        shell(
            "samtools view -Sb {sam} "
            "| samtools sort - -o {output.bam} -O BAM "
            "&& rm {sam}"
        )


rule unique:
    input:
        patterns["bam"],
    output:
        patterns["unique"],
    threads: 1
    resources:
        mem="1g",
        runtime="2h",
    params:
        # NOTE: the quality score chosen here should reflect the scores output
        # by the aligner used. For example, STAR uses 255 as max mapping
        # quality.
        extra="-q 20",
    shell:
        "samtools view -b {params.extra} {input} > {output}"


rule fastq_count:
    input:
        fastq="{sample_dir}/{sample}/{sample}{suffix}.fastq.gz",
    output:
        "{sample_dir}/{sample}/{sample}{suffix}.fastq.gz.libsize",
    threads: 1
    resources:
        mem="1g",
        runtime="2h",
    shell:
        "zcat {input} | echo $((`wc -l`/4)) > {output}"


rule bam_count:
    input:
        bam="{sample_dir}/{sample}/{suffix}.bam",
    output:
        "{sample_dir}/{sample}/{suffix}.bam.libsize",
    threads: 1
    resources:
        mem="2g",
        runtime="2h",
    shell:
        "samtools view -c {input} > {output}"


rule bam_index:
    input:
        bam="{prefix}.bam",
    output:
        bai="{prefix}.bam.bai",
    threads: 1
    resources:
        mem="2g",
        runtime="2h",
    shell:
        "samtools index {input} {output}"


rule markduplicates:
    input:
        bam=patterns["unique"],
    output:
        bam=patterns["markduplicates"]["bam"],
        metrics=patterns["markduplicates"]["metrics"],
    log:
        patterns["markduplicates"]["bam"] + ".log",
    threads: 1
    resources:
        mem="32g",
        disk="100g",
        runtime="2h",
    params:
        java_args="-Xmx20g",  # [disable for test]
        # java_args='-Xmx2g'  # [enable for test]
    shell:
        "picard "
        "{params.java_args} "
        "MarkDuplicates "
        "INPUT={input.bam} "
        "OUTPUT={output.bam} "
        "REMOVE_DUPLICATES=true "
        "METRICS_FILE={output.metrics} "
        "VALIDATION_STRINGENCY=LENIENT "
        "&> {log}"


rule merge_techreps:
    input:
        lambda wc: expand(
            patterns["markduplicates"]["bam"],
            sample=utils.get_techreps(sampletable, wc.label),
        ),
    output:
        bam=patterns["merged_techreps"],
        metrics=patterns["merged_techreps"] + ".metrics",
    log:
        patterns["merged_techreps"] + ".log",
    threads: 1
    resources:
        mem="32g",
        disk="100g",
        runtime="2h",
    params:
        java_args="-Xmx32g",  # [disable for test]
        # java_args='-Xmx2g'  # [enable for test]
    script:
        "../../scripts/merge_and_dedup.py"


if is_paired:

    rule collectinsertsizemetrics:
        input:
            bam=patterns["markduplicates"]["bam"],
        output:
            pdf=patterns["collectinsertsizemetrics"]["pdf"],
            metrics=patterns["collectinsertsizemetrics"]["metrics"],
        log:
            patterns["collectinsertsizemetrics"]["metrics"] + ".log",
        threads: 1
        resources:
            mem="32g",
            runtime="2h",
        params:
            java_args="-Xmx20g",  # [disable for test]
            # java_args='-Xmx2g'  # [enable for test]
        shell:
            "picard "
            "{params.java_args} "
            "CollectInsertSizeMetrics "
            "I={input.bam} "
            "O={output.metrics} "
            "H={output.pdf} "
            "&> {log} "


rule bigwig:
    input:
        bam=patterns["merged_techreps"],
        bai=patterns["merged_techreps"] + ".bai",
    output:
        patterns["bigwig"],
    log:
        patterns["bigwig"] + ".log",
    threads: 1
    resources:
        mem="16g",
        runtime="2h",
    shell:
        "bamCoverage "
        "--bam {input.bam} "
        "-o {output} "
        "-p {threads} "
        "--minMappingQuality 20 "
        "--ignoreDuplicates "
        # Can't use the CPM normalization for testing due to <1000 reads total
        # in example data; keep uncommented when running in production
        "--normalizeUsing CPM "  # [disable for test]
        "--extendReads 300 "
        "&> {log}"


rule fingerprint:
    """
    Runs deepTools plotFingerprint to assess how well the ChIP experiment
    worked.

    Note: uses the merged techreps.
    """
    input:
        bams=lambda wc: expand(patterns["merged_techreps"], label=wc.ip_label),
        control=lambda wc: expand(
            patterns["merged_techreps"],
            label=chipseq.merged_input_for_ip(sampletable, wc.ip_label),
        ),
        bais=lambda wc: expand(patterns["merged_techreps"] + ".bai", label=wc.ip_label),
        control_bais=lambda wc: expand(
            patterns["merged_techreps"] + ".bai",
            label=chipseq.merged_input_for_ip(sampletable, wc.ip_label),
        ),
    output:
        plot=patterns["fingerprint"]["plot"],
        raw_counts=patterns["fingerprint"]["raw_counts"],
        metrics=patterns["fingerprint"]["metrics"],
    threads: 8
    log:
        patterns["fingerprint"]["metrics"] + ".log",
    threads: 1
    resources:
        mem="32g",
        runtime="2h",
    run:
        if len(input.control) == 0:
            jsdsample_arg = ""
        else:
            jsdsample_arg = "--JSDsample " + str(input.control)
        shell(
            "plotFingerprint "
            "--bamfiles {input.bams} "
            "-p {threads} "
            # The JSDsample argument is disabled for testing as it dramatically
            # increases the run time.
            "{jsdsample_arg} "  # [disable for test]
            "--smartLabels "
            "--extendReads=300 "
            "--skipZeros "
            "--outQualityMetrics {output.metrics} "
            "--outRawCounts {output.raw_counts} "
            "--plotFile {output.plot} "
            # Default is 500k; use fewer to speed up testing:
            # '--numberOfSamples 50 '  # [enable for test]
            "&> {log} "
            '&& sed -i "s/NA/0.0/g" {output.metrics} '
        )



rule macs2:
    """
    Run the macs2 peak caller
    """
    input:
        ip=lambda wc: expand(
            patterns["merged_techreps"],
            label=chipseq.samples_for_run(config, wc.macs2_run, "macs2", "ip"),
        ),
        control=lambda wc: expand(
            patterns["merged_techreps"],
            label=chipseq.samples_for_run(config, wc.macs2_run, "macs2", "control"),
        ),
        chromsizes=rules.chromsizes.output,
    output:
        bed=patterns["peaks"]["macs2"],
    resources:
        mem="16g",
        runtime="2h",
    log:
        patterns["peaks"]["macs2"] + ".log",
    params:
        block=lambda wc: chipseq.block_for_run(config, wc.macs2_run, "macs2"),
    script:
        "../../scripts/macs2_callpeak.py"


rule epic2:
    """
    Run the epic2 peak caller
    """
    input:
        ip=lambda wc: expand(
            patterns["merged_techreps"],
            label=chipseq.samples_for_run(config, wc.epic2_run, "epic2", "ip"),
        ),
        control=lambda wc: expand(
            patterns["merged_techreps"],
            label=chipseq.samples_for_run(config, wc.epic2_run, "epic2", "control"),
        ),
        bai=lambda wc: expand(
            patterns["merged_techreps"] + ".bai",
            label=chipseq.samples_for_run(config, wc.epic2_run, "epic2", "ip"),
        )
        + expand(
            patterns["merged_techreps"] + ".bai",
            label=chipseq.samples_for_run(config, wc.epic2_run, "epic2", "control"),
        ),
        chromsizes=rules.chromsizes.output,
    output:
        bed=patterns["peaks"]["epic2"],
    resources:
        mem="16g",
        runtime="2h",
    log:
        patterns["peaks"]["epic2"] + ".log",
    params:
        block=lambda wc: chipseq.block_for_run(config, wc.epic2_run, "epic2"),
        is_paired=is_paired,
    script:
        "../../scripts/epic2.py"


rule bed_to_bigbed:
    """
    Convert BED to bigBed
    """
    input:
        bed="{prefix}.bed",
        chromsizes=rules.chromsizes.output,
    output:
        "{prefix}.bigbed",
    resources:
        mem="2g",
        runtime="2h",
    log:
        "{prefix}.bigbed.log",
    script:
        "../../scripts/bed_to_bigbed.py"


rule multibigwigsummary:
    """
    Summarize the bigWigs across genomic bins
    """
    input:
        expand(patterns["bigwig"], label=sampletable.label),
    output:
        npz=patterns["multibigwigsummary"]["npz"],
        tab=patterns["multibigwigsummary"]["tab"],
    threads: 16
    resources:
        mem="16g",
        runtime="2h",
    run:
        # from the input files, figure out the sample name.
        labels = " ".join([i.split("/")[-2] for i in input])
        shell(
            "multiBigwigSummary "
            "bins "
            "-b {input} "
            "--labels {labels} "
            "--numberOfProcessors {threads} "
            "-out {output.npz} "
            "--outRawCounts {output.tab}"
        )


rule plotcorrelation:
    """
    Plot a heatmap of correlations across all samples
    """
    input:
        patterns["multibigwigsummary"]["npz"],
    output:
        heatmap=patterns["plotcorrelation"]["heatmap"],
        tab=patterns["plotcorrelation"]["tab"],
    resources:
        mem="2g",
        runtime="2h",
    shell:
        "plotCorrelation "
        "--corData {input} "
        "--corMethod spearman "
        "--whatToPlot heatmap "
        "--plotFile {output.heatmap} "
        "--colorMap Reds "
        "--outFileCorMatrix {output.tab}"
        # NOTE: if you're expecting negative correlation, try a divergent
        # colormap and setting the min/max to ensure that the colomap is
        # centered on zero:
        # '--colorMap RdBu_r '
        # '--zMin -1 '
        # '--zMax 1 '


rule idxstats:
    input:
        bam=patterns["markduplicates"]["bam"],
        bai=patterns["markduplicates"]["bam"] + ".bai",
    output:
        txt=patterns["samtools"]["idxstats"],
    resources:
        mem="16g",
        runtime="2h",
    log:
        patterns["samtools"]["idxstats"] + ".log",
    shell:
        "samtools idxstats {input.bam} 2> {log} 1> {output.txt}"


rule flagstat:
    input:
        bam=patterns["markduplicates"]["bam"],
        bai=patterns["markduplicates"]["bam"] + ".bai",
    output:
        patterns["samtools"]["flagstat"],
    resources:
        mem="8g",
        runtime="2h",
    log:
        patterns["samtools"]["flagstat"] + ".log",
    shell:
        "samtools flagstat {input.bam} > {output}"


rule samtools_stats:
    input:
        bam=patterns["markduplicates"]["bam"],
        bai=patterns["markduplicates"]["bam"] + ".bai",
    output:
        patterns["samtools"]["stats"],
    resources:
        mem="8g",
        runtime="2h",
    log:
        patterns["samtools"]["stats"] + ".log",
    shell:
        "samtools stats {input.bam} > {output}"


rule multiqc:
    input:
        expand(patterns["bam"], sample=SAMPLES),
        expand(patterns["fastqc"]["raw"], sample=SAMPLES),
        expand(patterns["fastqc"]["cutadapt"], sample=SAMPLES),
        expand(patterns["fastqc"]["bam"], sample=SAMPLES),
        expand(patterns["bigwig"], label=sampletable.label),
        expand(patterns["samtools"]["idxstats"], sample=SAMPLES),
        expand(patterns["samtools"]["flagstat"], sample=SAMPLES),
        expand(patterns["samtools"]["stats"], sample=SAMPLES),
        expand(patterns["merged_techreps"], label=sampletable.label),
        expand(
            patterns["fingerprint"]["metrics"],
            ip_label=sampletable.loc[sampletable.antibody != "input", "label"],
        ),
        expand(patterns["collectinsertsizemetrics"], sample=SAMPLES)
        if is_paired
        else [],
        [v["bigbed"] for v in peaks.values()],
        patterns["multibigwigsummary"]["tab"],
        patterns["plotcorrelation"]["tab"],
        config="config/multiqc_config.yaml",
    output:
        patterns["multiqc"],
    log:
        patterns["multiqc"] + ".log",
    threads: 1
    resources:
        mem="2g",
        runtime="2h",
    run:
        analysis_directory = "data"
        outdir = os.path.dirname(output[0])
        basename = os.path.basename(output[0])
        shell(
            "LC_ALL=en_US.utf8 LC_LANG=en_US.utf8 "
            "multiqc "
            "--quiet "
            "--outdir {outdir} "
            "--force "
            "--filename {basename} "
            "--config {input.config} "
            "{analysis_directory} "
            "&> {log} "
        )
