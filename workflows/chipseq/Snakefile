import sys
sys.path.insert(0, srcdir('../..'))
import os
from textwrap import dedent
import yaml
import tempfile
import pandas as pd
import numpy as np
import pybedtools
from lcdblib.snakemake import helpers, aligners
from lcdblib.utils import utils
from lib import common, chipseq
from lib.patterns_targets import ChIPSeqConfig

# ----------------------------------------------------------------------------
#
# Search for the string "NOTE:" to look for points of configuration that might
# be helpful for your experiment.
#
# ----------------------------------------------------------------------------


configfile: 'config/config.yaml'
include: '../references/Snakefile'

shell.prefix('set -euo pipefail; export TMPDIR={};'.format(common.tempdir_for_biowulf()))
shell.executable('/bin/bash')

c = ChIPSeqConfig(config, 'config/chipseq_patterns.yaml')


def wrapper_for(path):
    return 'file:' + os.path.join('../..','wrappers', 'wrappers', path)

# ----------------------------------------------------------------------------
# RULES
# ----------------------------------------------------------------------------
# See "patterns and targets" in the documentation for what's going on here.
final_targets = utils.flatten((
    c.targets['bam'],
    utils.flatten(c.targets['fastqc']),
    utils.flatten(c.targets['libsizes']),
    [c.targets['fastq_screen']],
    [c.targets['libsizes_table']],
    [c.targets['multiqc']],
    utils.flatten(c.targets['markduplicates']),
    utils.flatten(c.targets['bigwig']),
    utils.flatten(c.targets['peaks']),
    utils.flatten(c.targets['merged_techreps']),
    utils.flatten(c.targets['fingerprint']),
    utils.flatten(c.targets['bigbed']),
    utils.flatten(c.targets['multibigwigsummary']),
    utils.flatten(c.targets['plotcorrelation']),
))

if 'merged_bigwigs' in config:
    final_targets.extend(utils.flatten(c.targets['merged_bigwig']))

rule targets:
    """
    Final targets to create
    """
    input: final_targets


if 'orig_filename' in c.sampletable.columns:
    rule symlinks:
        """
        Symlinks files over from original filename
        """
        input: lambda wc: c.sampletable.set_index(c.sampletable.columns[0])['orig_filename'].to_dict()[wc.sample]
        output: c.patterns['fastq']
        run:
            utils.make_relative_symlink(input[0], output[0])

    rule symlink_targets:
        input: c.targets['fastq']

if 'Run' in c.sampletable.columns and sum(c.sampletable['Run'].str.startswith('SRR')) > 1:

    # Convert the sampletable to be indexed by the first column, for
    # convenience in generating the input/output filenames.
    _st = c.sampletable.set_index(c.sampletable.columns[0])

    rule fastq_dump:
        output:
            c.patterns['fastq']
        run:
            srr = _st.loc[wildcards.sample, 'Run']
            shell(
                'fastq-dump '
                '{srr} '
                '-Z '
                '| gzip -c > {output}.tmp '
                '&& mv {output}.tmp {output} '
            )



rule cutadapt:
    """
    Run cutadapt
    """
    input:
        fastq=c.patterns['fastq']
    output:
        fastq=c.patterns['cutadapt']
    log:
        c.patterns['cutadapt'] + '.log'
    params:
        # NOTE: Change cutadapt params here.
        extra='-a file:../../include/adapters.fa -q 20 --minimum-length=25'
    wrapper:
        wrapper_for('cutadapt')


rule fastqc:
    """
    Run FastQC
    """
    input: '{sample_dir}/{sample}/{sample}{suffix}'
    output:
        html='{sample_dir}/{sample}/fastqc/{sample}{suffix}_fastqc.html',
        zip='{sample_dir}/{sample}/fastqc/{sample}{suffix}_fastqc.zip',
    wrapper:
        wrapper_for('fastqc')


rule bowtie2:
    """
    Map reads with Bowtie2
    """
    input:
        fastq=rules.cutadapt.output.fastq,
        index=[refdict[c.organism][config['aligner']['tag']]['bowtie2']]
    output:
        bam=c.patterns['bam']
    log:
        c.patterns['bam'] + '.log'
    params:
        samtools_view_extra='-F 0x04'
    # NOTE:
    # When running on a cluster, number of threads here should match the
    # resources requested in the cluster config.
    threads: 8
    wrapper:
        wrapper_for('bowtie2/align')


rule unique:
    """
    Remove multimappers
    """
    input:
        c.patterns['bam']
    output:
        c.patterns['unique']
    params:
        extra="-b -q 20"
    shell:
        'samtools view -b -q 20 {input} > {output}'


rule fastq_count:
    """
    Count reads in a FASTQ file
    """
    input:
        fastq='{sample_dir}/{sample}/{sample}{suffix}.fastq.gz'
    output:
        count='{sample_dir}/{sample}/{sample}{suffix}.fastq.gz.libsize'
    shell:
        'zcat {input} | echo $((`wc -l`/4)) > {output}'


rule bam_count:
    """
    Count reads in a BAM file
    """
    input:
        bam='{sample_dir}/{sample}/{sample}{suffix}.bam'
    output:
        count='{sample_dir}/{sample}/{sample}{suffix}.bam.libsize'
    shell:
        'samtools view -c {input} > {output}'


rule bam_index:
    """
    Index a BAM
    """
    input:
        bam='{prefix}.bam'
    output:
        bai='{prefix}.bam.bai'
    shell:
        'samtools index {input} {output}'


def fastq_screen_references():
    """
    Returns the Bowtie2 indexes for the configured references from the
    `fastq_screen:` section of the config
    """
    refs = {}
    for i in config['fastq_screen']:
        refs[i['label']] = c.refdict[i['organism']][i['tag']]['bowtie2']
    return refs


rule fastq_screen:
    """
    Run fastq_screen to look for contamination from other genomes
    """
    input:
        **fastq_screen_references(),
        fastq=rules.cutadapt.output.fastq,
    output:
        txt=c.patterns['fastq_screen']
    log:
        c.patterns['fastq_screen'] + '.log'
    params: subset=100000
    wrapper:
        wrapper_for('fastq_screen')


rule libsizes_table:
    """
    Aggregate fastq and bam counts in to a single table
    """
    input:
        utils.flatten(c.targets['libsizes'])
    output:
        json=c.patterns['libsizes_yaml'],
        tsv=c.patterns['libsizes_table']
    run:
        def sample(f):
            return os.path.basename(os.path.dirname(f))

        def million(f):
            return float(open(f).read()) / 1e6

        def stage(f):
            return os.path.basename(f).split('.', 1)[1].replace('.gz', '').replace('.count', '')

        df = pd.DataFrame(dict(filename=list(map(str, input))))
        df['sample'] = df.filename.apply(sample)
        df['million'] = df.filename.apply(million)
        df['stage'] = df.filename.apply(stage)
        df = df.set_index('filename')
        df = df.pivot('sample', columns='stage', values='million')

        # make nicer column names
        convert = {
            'fastq.libsize': 'stage1_raw',
            'cutadapt.fastq.libsize' : 'stage2_trimmed',
            'cutadapt.bam.libsize': 'stage3_aligned',
            'cutadapt.unique.bam.libsize': 'stage4_unique',
            'cutadapt.unique.nodups.bam.libsize': 'stage5_nodups',
        }

        df.columns = [convert[i] for i in df.columns]

        df.to_csv(output.tsv, sep='\t')
        y = {
            'id': 'libsizes_table',
            'section_name': 'Library sizes',
            'description': 'Library sizes at various stages of the pipeline',
            'plot_type': 'table',
            'pconfig': {
                'id': 'libsizes_table_table',
                'title': 'Library size table',
                'min': 0
            },
            'data': yaml.load(df.transpose().to_json()),
        }
        with open(output.json, 'w') as fout:
            yaml.dump(y, fout, default_flow_style=False)


rule multiqc:
    """
    Aggregate various QC stats and logs into a single HTML report with MultiQC
    """
    # NOTE: if you add more rules and want MultiQC to pick up the output, best
    # to add outputs from those rules to the inputs here.
    input:
        files=(
            utils.flatten(c.targets['fastqc']) +
            utils.flatten(c.targets['libsizes_yaml']) +
            utils.flatten(c.targets['cutadapt']) +
            utils.flatten(c.targets['bam']) +
            utils.flatten(c.targets['markduplicates']) +
            utils.flatten(c.targets['fingerprint']) +
            utils.flatten(c.targets['peaks']) +
            utils.flatten(c.targets['fastq_screen'])
        ),
        config='config/multiqc_config.yaml'
    output:
        c.targets['multiqc']
    log:
        c.targets['multiqc'][0] + '.log'
    run:
        analysis_directory = set([os.path.dirname(i) for i in input])
        outdir = os.path.dirname(c.targets['multiqc'][0])
        basename = os.path.basename(c.targets['multiqc'][0])
        shell(
            'LC_ALL=en_US.UTF.8 LC_LANG=en_US.UTF-8 '
            'multiqc '
            '--quiet '
            '--outdir {outdir} '
            '--force '
            '--filename {basename} '
            '--config {input.config} '
            '{analysis_directory} '
            '&> {log} '
        )


rule markduplicates:
    """
    Mark or remove PCR duplicates with Picard MarkDuplicates
    """
    input:
        bam=c.patterns['unique']
    output:
        bam=c.patterns['markduplicates']['bam'],
        metrics=c.patterns['markduplicates']['metrics']
    log:
        c.patterns['markduplicates']['bam'] + '.log'
    params:
        # NOTE: Be careful with the memory here; make sure you have enough
        # and/or it matches the resources you're requesting in the cluster
        # config.
        java_args='-Xmx20g'
        # java_args='-Xmx2g'  # [TEST SETTINGS -1]
    shell:
        'picard '
        '{params.java_args} '
        'MarkDuplicates '
        'INPUT={input.bam} '
        'OUTPUT={output.bam} '
        'REMOVE_DUPLICATES=true '
        'METRICS_FILE={output.metrics} '
        '&> {log}'


rule merge_techreps:
    """
    Technical replicates are merged and then re-deduped.

    If there's only one technical replicate, its unique, nodups bam is simply
    symlinked.
    """
    input:
        lambda wc: expand(
            c.patterns['markduplicates']['bam'],
            sample=common.get_techreps(c.sampletable, wc.label),
        )
    output:
        bam=c.patterns['merged_techreps'],
        metrics=c.patterns['merged_techreps'] + '.metrics'
    log:
        c.patterns['merged_techreps'] + '.log'
    params:
        # NOTE: Be careful with the memory here; make sure you have enough
        # and/or it matches the resources you're requesting in the cluster
        # config.
        java_args='-Xmx32g'
        # java_args='-Xmx2g'  # [TEST SETTINGS -1]
    wrapper:
        wrapper_for('combos/merge_and_dedup')


rule bigwig:
    """
    Create a bigwig.

    See note below about normalizing!
    """
    input:
        bam=c.patterns['merged_techreps'],
        bai=c.patterns['merged_techreps'] + '.bai',
    output:
        c.patterns['bigwig']
    log:
        c.patterns['bigwig'] + '.log'
    shell:
        'bamCoverage '
        '--bam {input.bam} '
        '-o {output} '
        '-p {threads} '
        '--minMappingQuality 20 '
        '--ignoreDuplicates '
        # Can't use the CPM normalization for testing due to <1000 reads total
        # in example data; keep uncommented when running in production
        # [TEST SETTINGS +1]
        '--normalizeUsing CPM '
        '--extendReads 300 '
        '&> {log}'


rule fingerprint:
    """
    Runs deepTools plotFingerprint to assess how well the ChIP experiment
    worked.

    Note: uses the merged techreps.
    """
    input:
        bams=lambda wc: expand(c.patterns['merged_techreps'], label=wc.ip_label),
        control=lambda wc: expand(c.patterns['merged_techreps'], label=chipseq.merged_input_for_ip(c.sampletable, wc.ip_label)),
        bais=lambda wc: expand(c.patterns['merged_techreps'] + '.bai', label=wc.ip_label),
        control_bais=lambda wc: expand(c.patterns['merged_techreps'] + '.bai', label=chipseq.merged_input_for_ip(c.sampletable, wc.ip_label)),
    output:
        plot=c.patterns['fingerprint']['plot'],
        raw_counts=c.patterns['fingerprint']['raw_counts'],
        metrics=c.patterns['fingerprint']['metrics']
    threads: 8
    log: c.patterns['fingerprint']['metrics'] + '.log'
    shell:
        'plotFingerprint '
        '--bamfiles {input.bams} '
        '-p {threads} '
        # The JSDsample argument is disabled for testing as it dramatically
        # increases the run time.
        # [TEST SETTINGS +1]
        '--JSDsample {input.control} '
        '--smartLabels '
        '--extendReads=300 '
        '--skipZeros '
        '--outQualityMetrics {output.metrics} '
        '--outRawCounts {output.raw_counts} '
        '--plotFile {output.plot} '
        # Default is 500k; use fewer to speed up testing:
        # '--numberOfSamples 50 '  # [TEST SETTINGS ]
        '&> {log} '
        '&& sed -i "s/NA/0.0/g" {output.metrics} '


rule sicer:
    """
    Run the SICER peak caller
    """
    input:
        ip=lambda wc:
            expand(
                c.patterns['merged_techreps'],
                label=chipseq.samples_for_run(config, wc.sicer_run, 'sicer', 'ip'),
            ),
        control=lambda wc:
            expand(
                c.patterns['merged_techreps'],
                label=chipseq.samples_for_run(config, wc.sicer_run, 'sicer', 'control'),
            ),
        chromsizes=refdict[c.organism][config['aligner']['tag']]['chromsizes'],
    output:
        bed=c.patterns['peaks']['sicer']
    log:
        c.patterns['peaks']['sicer'] + '.log'
    params:
        block=lambda wc: chipseq.block_for_run(config, wc.sicer_run, 'sicer')
    wrapper:
        wrapper_for('sicer')

rule macs2:
    """
    Run the macs2 peak caller
    """
    input:
        ip=lambda wc:
            expand(
                c.patterns['merged_techreps'],
                label=chipseq.samples_for_run(config, wc.macs2_run, 'macs2', 'ip'),
            ),
        control=lambda wc:
            expand(
                c.patterns['merged_techreps'],
                label=chipseq.samples_for_run(config, wc.macs2_run, 'macs2', 'control'),
            ),
        chromsizes=refdict[c.organism][config['aligner']['tag']]['chromsizes'],
    output:
        bed=c.patterns['peaks']['macs2']
    log:
        c.patterns['peaks']['macs2'] + '.log'
    params:
        block=lambda wc: chipseq.block_for_run(config, wc.macs2_run, 'macs2')
    wrapper:
        wrapper_for('macs2/callpeak')


rule spp:
    """
    Run the SPP peak caller
    """
    input:
        ip=lambda wc:
            expand(
                c.patterns['merged_techreps'],
                label=chipseq.samples_for_run(config, wc.spp_run, 'spp', 'ip'),
            ),
        control=lambda wc:
            expand(
                c.patterns['merged_techreps'],
                label=chipseq.samples_for_run(config, wc.spp_run, 'spp', 'control'),
            ),
        chromsizes=refdict[c.organism][config['aligner']['tag']]['chromsizes'],
    output:
        bed=c.patterns['peaks']['spp'],
        enrichment_estimates=c.patterns['peaks']['spp'] + '.est.wig',
        smoothed_enrichment_mle=c.patterns['peaks']['spp'] + '.mle.wig',
        rdata=c.patterns['peaks']['spp'] + '.RData'
    log:
        c.patterns['peaks']['spp'] + '.log'
    params:
        block=lambda wc: chipseq.block_for_run(config, wc.spp_run, 'spp'),
        keep_tempfiles=False,
        # NOTE: Be careful with the memory here; make sure you have enough
        # and/or it matches the resources you're requesting in the cluster
        # config.
        java_args='-Xmx24g',
        # java_args='-Xmx2g',  # [TEST SETTINGS -1]
    threads: 2
    wrapper:
        wrapper_for('spp')


rule bed_to_bigbed:
    """
    Convert BED to bigBed
    """
    input:
        bed='{prefix}.bed',
        chromsizes=refdict[c.organism][config['aligner']['tag']]['chromsizes']
    output: '{prefix}.bigbed'
    run:
        # Based on the filename, identify the algorithm. Based on the contents,
        # identify the format.
        algorithm = os.path.basename(os.path.dirname(input.bed))
        kind = chipseq.detect_peak_format(input.bed)


        # bedToBigBed doesn't handle zero-size files
        if os.stat(input.bed).st_size == 0:
            shell("touch {output}")
        else:
            if kind == 'narrowPeak':
                _as = '../../include/bigNarrowPeak.as'
                _type = 'bed6+4'
                names=[
                    'chrom', 'chromStart', 'chromEnd', 'name', 'score',
                    'strand', 'signalValue', 'pValue', 'qValue', 'peak']
            elif kind == 'broadPeak':
                _as = '../../include/bigBroadPeak.as'
                _type = 'bed6+3'
                names=[
                    'chrom', 'chromStart', 'chromEnd', 'name', 'score',
                    'strand', 'signalValue', 'pValue', 'qValue']
            else:
                raise ValueError("Unhandled format for {0}".format(input.bed))

            df = pd.read_table(input.bed, index_col=False, names=names)
            df['score'] = df['score'] - df['score'].min()
            df['score'] = (df['score'] / df['score'].max()) * 1000
            df['score'] = df['score'].replace([np.inf, -np.inf], np.nan).fillna(0)
            df['score'] = df['score'].astype(int)
            df.to_csv(output[0] + '.tmp', sep='\t', index=False, header=False)

            shell('bedToBigBed -as={_as} -type={_type} {output}.tmp {input.chromsizes} {output}')
            shell('rm {output}.tmp')


rule multibigwigsummary:
    """
    Summarize the bigWigs across genomic bins
    """
    input:
        c.targets['bigwig']
    output:
        npz=c.targets['multibigwigsummary']['npz'],
        tab=c.targets['multibigwigsummary']['tab']
    run:
        # from the input files, figure out the sample name.
        labels = ' '.join([i.split('/')[-2] for i in input])
        shell(
            'multiBigwigSummary '
            'bins '
            '-b {input} '
            '--labels {labels} '
            '-out {output.npz} '
            '--outRawCounts {output.tab}'
        )


rule plotcorrelation:
    """
    Plot a heatmap of correlations across all samples
    """
    input:
        c.targets['multibigwigsummary']['npz']
    output:
        heatmap=c.targets['plotcorrelation']['heatmap'],
        tab=c.targets['plotcorrelation']['tab']
    shell:
        'plotCorrelation '
        '--corData {input} '
        '--corMethod spearman '
        '--whatToPlot heatmap '
        '--plotFile {output.heatmap} '
        '--colorMap viridis '
        '--outFileCorMatrix {output.tab}'

        # NOTE: if you're expecting negative correlation, try a divergent
        # colormap and setting the min/max to ensure that the colomap is
        # centered on zero:
        # '--colorMap RdBu_r '
        # '--zMin -1 '
        # '--zMax 1 '

if 'merged_bigwigs' in config:
    rule merge_bigwigs:
        """
        Merge together bigWigs as specified in the config ("merged_bigwigs"
        section).
        """
        input:
            bigwigs=lambda wc: expand(
                c.patterns['bigwig'],
                label=config['merged_bigwigs'][wc.merged_bigwig_label],
            ),
            chromsizes=refdict[c.organism][config['aligner']['tag']]['chromsizes'],
        output:
            c.patterns['merged_bigwig']
        log:
            c.patterns['merged_bigwig'] + '.log'
        wrapper:
            wrapper_for('average-bigwigs')

# vim: ft=python
