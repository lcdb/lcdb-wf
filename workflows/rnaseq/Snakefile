import os
import yaml
import tempfile
import pandas as pd

configfile: 'config/config.yaml'

include: '../../rules/references.smk'
include: '../../rules/common.smk'

REFERENCES = config.get('reference_dir', '../../references')

sampletable = pd.read_table(config["sampletable"], sep="\t")
_st = c.sampletable.set_index(c.sampletable.columns[0])
is_paired = detect_layout(sampletable) == "PE"
is_sra = detect_sra(sampletable)
n = ["1", "2"] if is_paired else ["1"]
SAMPLES = sampletable.iloc[:, 0].values

# TODO: moved utils.py over to common.smk; not sure if this means that postprocessing will fail or not...

wildcard_constraints:
    n = '[1,2]',
    sample = '|'.join(SAMPLES)

localrules: symlinks, symlink_targets

rule all:
    input:
        'data/rnaseq_aggregation/multiqc.html',

if is_sra:
    include: '../../rules/sra.smk'


def orig_for_sample(wc):
    """
    Given a sample, returns either one or two original fastq files
    depending on whether the library was single- or paired-end.
    """
    if is_paired:
        return _st.loc[wc.sample, ['orig_filename', 'orig_filename_R2']]
    return _st.loc[wc.sample, ['orig_filename']]


rule symlinks:
    input:
        lambda wc: _st.loc[wc.sample, ['orig_filename', 'orig_filename_R2']] if is_paired
        else _st.loc[wc.sample, ['orig_filename']]
    output:
        expand('data/rnaseq_samples/{sample}/{sample}_R{n}.fastq.gz', sample=SAMPLES, n=n)
    threads: 1
    resources:
        mem_mb=100,
        runtime=10,
    run:
        assert len(output) == len(input), (input, output)
        for src, linkname in zip(input, output):
            make_relative_symlink(src, linkname)


rule symlink_targets:
    input: c.targets['fastq']

# This can be set at the command line with --config strand_check_reads=1000
config.setdefault('strand_check_reads', 1e5)
include: '../../rules/strand_check.smk'

rule cutadapt:
    input:
        fastq=expand('data/rnaseq_samples/{sample}/{sample}_R{n}.fastq.gz', sample=SAMPLES, n=n)
    output:
        fastq=expand('data/rnaseq_samples/{sample}/{sample}_R{n}.cutadapt.fastq.gz', sample=SAMPLES, n=n)
    log:
        'data/rnaseq_samples/{sample}/{sample}_R1.fastq.gz'
    threads: 6
    resources:
        mem_mb=gb(2),
        runtime=autobump(hours=2)
    params:
        extra=(
            "--nextseq-trim 20 "
            "--overlap 6 "
            "--minimum-length 25 "
            "-a AGATCGGAAGAGCACACGTCTGAACTCCAGTCA "
        ) + "-A AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT " if is_paired else ""
    run:
        if c.is_paired:
            shell(
                "cutadapt "
                "-o {output[0]} "
                "-p {output[1]} "
                "-j {threads} "
                "{params.extra} "
                "{input.fastq[0]} "
                "{input.fastq[1]} "
                "&> {log}"
            )
        else:
            shell(
                "cutadapt "
                "-o {output[0]} "
                "-j {threads} "
                "{params.extra} "
                "{input.fastq[0]} "
                "&> {log}"
            )

# TODO: rm wrapper
rule fastqc:
    input:
        '{sample_dir}/{sample}/{sample}{suffix}'
    threads:
        6
    output:
        html='{sample_dir}/{sample}/fastqc/{sample}{suffix}_fastqc.html',
        zip='{sample_dir}/{sample}/fastqc/{sample}{suffix}_fastqc.zip',
    resources:
        mem_mb=gb(8),
        runtime=autobump(hours=2)
    script:
        wrapper_for('fastqc/wrapper.py')


if config['aligner'] == 'hisat2':
    rule hisat2:
        input:
            # TODO: make sure this works
            fastq=(
                 'data/rnaseq_samples/{sample}/{sample}_R1.fastq.gz',
                 'data/rnaseq_samples/{sample}/{sample}_R2.fastq.gz',
            ) if is_paired else (
                 'data/rnaseq_samples/{sample}/{sample}_R1.fastq.gz',
            ),
            index=rules.hisat2_index.output,
        output:
            bam=temporary(c.patterns['bam'])
        log:
            c.patterns['bam'] + '.log'
        threads: 6
        resources:
            mem_mb=gb(32),
            runtime=autobump(hours=8)
        params:
            extra=""
        run:

            prefix = os.path.commonprefix(input.index).rstrip(".")
            sam = output.bam.replace('.bam', '.sam')


            if c.is_paired:
                assert len(input.fastq) == 2
                fastqs = '-1 {0} -2 {1} '.format(*input.fastq)
            else:
                assert len(input.fastq) == 1
                fastqs = '-U {0} '.format(input.fastq)

            shell(
                "hisat2 "
                "-x {prefix} "
                "{fastqs} "
                '--no-unal '
                "--threads {threads} "
                "-S {sam} "
                "> {log} 2>&1"
            )

            shell(
                "samtools view -Sb {sam} "
                "| samtools sort - -o {output.bam} -O BAM "
                "&& rm {sam}"
            )

if config['aligner'].startswith('star'):

    # STAR can be run in 1-pass or 2-pass modes. Since we may be running it
    # more than once in almost the same way, we pull out the shell command here
    # and use it below.
    STAR_CMD = (
        'STAR '
        '--runThreadN {threads} '
        '--genomeDir {genomedir} '
        '--readFilesIn {input.fastq} '
        '--readFilesCommand zcat '
        '--outFileNamePrefix {prefix} '
        '{params.extra} '
    )
    STAR_PARAMS = (
        # NOTE: The STAR docs indicate that the following parameters are
        # standard options for ENCODE long-RNA-seq pipeline.  Comments are from
        # the STAR docs.
        '--outFilterType BySJout '               # reduces number of spurious junctions
        '--outFilterMultimapNmax 20 '            # if more than this many multimappers, consider unmapped
        '--alignSJoverhangMin 8 '                # min overhang for unannotated junctions
        '--alignSJDBoverhangMin 1 '              # min overhang for annotated junctions
        '--outFilterMismatchNmax 999 '           # max mismatches per pair
        '--outFilterMismatchNoverReadLmax 0.04 ' # max mismatches per pair relative to read length
        '--alignIntronMin 20 '                   # min intron length
        '--alignIntronMax 1000000 '              # max intron length
        '--alignMatesGapMax 1000000 '            # max distance between mates
        '--outSAMunmapped None '                 # do not report aligned reads in output
    )
    logfile_extensions = ['Log.progress.out', 'Log.out', 'Log.final.out', 'Log.std.out']

if config['aligner'] == 'star':

    rule star:
        """
        Align with STAR (1-pass mode)
        """
        input:
            fastq=fill_r1_r2(c.sampletable, c.patterns['cutadapt']),
            index=rules.star_index.output,
            annotation=REFERENCES + "/annotation.gtf"
        output:
            bam=temporary(c.patterns['bam']),
            sjout=temporary(c.patterns['bam'].replace('.bam', '.star.SJ.out.tab')),
        log:
            c.patterns['bam'].replace('.bam', '.star.bam.log')
        threads: 16
        resources:
            mem_mb=gb(64),
            runtime=autobump(hours=8)
        params:
            extra=STAR_PARAMS

        run:
            genomedir = os.path.dirname(input.index[0])
            outdir = os.path.dirname(output[0])
            prefix = output.bam.replace('.bam', '.star.')
            shell(
                STAR_CMD + (
                    '--outSAMtype BAM SortedByCoordinate '
                    '--outStd BAM_SortedByCoordinate > {output.bam} '
                    '2> {log} '
                )
            )

            # move various hard-coded log files to log directory
            logfiles = expand(prefix + '{ext}', ext=logfile_extensions)
            shell('mkdir -p {outdir}/star_logs '
                  '&& mv {logfiles} {outdir}/star_logs')

if config['aligner'] == 'star-twopass':

    rule star_pass1:
        """
        First pass of alignment with STAR to get the junctions
        """
        input:
            fastq=fill_r1_r2(c.sampletable, c.patterns['cutadapt']),
            index=rules.star_index.output,
            annotation=REFERENCES + "/annotation.gtf"
        output:
            sjout=temporary(c.patterns['bam'].replace('.bam', '.star-pass1.SJ.out.tab')),
        log:
            c.patterns['bam'].replace('.bam', '.star-pass1.bam.log')
        threads: 16
        resources:
            mem_mb=gb(64),
            runtime=autobump(hours=8)
        params:
            extra=STAR_PARAMS
        run:
            genomedir = os.path.dirname(input.index[0])
            outdir = os.path.dirname(output[0])
            prefix = output.sjout.replace('SJ.out.tab', '')
            shell(
                STAR_CMD +
                (
                    # In this first pass, we don't actually care about the
                    # alignment -- just the detected junctions. So we output
                    # the SAM to /dev/null.
                    '--outStd SAM > /dev/null '
                    '2> {log} '
                )
            )

            # move various hard-coded log files to log directory
            logfiles = expand(prefix + '{ext}', ext=logfile_extensions)
            shell('mkdir -p {outdir}/star-pass1_logs '
                  '&& mv {logfiles} {outdir}/star-pass1_logs')


    rule star_pass2:
        """
        Second pass of alignment with STAR using splice junctions across all
        samples to get the final BAM
        """
        input:
            sjout=expand(c.patterns['bam'].replace('.bam', '.star-pass1.SJ.out.tab'), sample=SAMPLES),
            fastq=fill_r1_r2(c.sampletable, c.patterns['cutadapt']),
            index=rules.star_index.output,
            annotation=REFERENCES + "/annotation.gtf"
        output:
            bam=temporary(c.patterns['bam']),
            sjout=temporary(c.patterns['bam'].replace('.bam', '.star-pass2.SJ.out.tab')),
        log:
            c.patterns['bam'].replace('.bam', '.star-pass2.bam.log')
        threads: 16
        resources:
            mem_mb=gb(64),
            runtime=autobump(hours=8)
        params:
            extra=STAR_PARAMS
        run:
            genomedir = os.path.dirname(input.index[0])
            outdir = os.path.dirname(output[0])
            prefix = output.bam.replace('.bam', '.star-pass2.')
            shell(
                STAR_CMD + (
                    # In contrast to pass 1, we will be keeping these BAMs --
                    # so sort them
                    '--outSAMtype BAM SortedByCoordinate '

                    # Splice junction databases from all samples in the first
                    # pass.
                    '--sjdbFileChrStartEnd {input.sjout} '
                    '--outStd BAM_SortedByCoordinate > {output.bam} '
                    '2> {log} '
                )
            )

            # move various hard-coded log files to log directory
            logfiles = expand(prefix + '{ext}', ext=logfile_extensions)
            shell('mkdir -p {outdir}/star-pass2_logs '
                  '&& mv {logfiles} {outdir}/star-pass2_logs')

            shell('rm -r {prefix}_STARgenome')


rule rRNA:
    input:
        fastq=render_r1_only(c.patterns['cutadapt']),
        index=multiext(
            REFERENCES + "/bowtie2/rrna",
            ".1.bt2",
            ".2.bt2",
            ".3.bt2",
            ".4.bt2",
            ".rev.1.bt2",
            ".rev.2.bt2",
            ".fa",
        ),
    output:
        bam=temporary(c.patterns['rrna']['bam'])
    log:
        c.patterns['rrna']['bam'] + '.log'
    threads: 6
    resources:
        mem_mb=gb(2),
        runtime=autobump(hours=2)
    params:
        extra=(
            '-k 1 '       # NOTE: we only care if >=1 mapped
            '--no-unal '  # NOTE: suppress unaligned reads
        )

    run:
        prefix = os.path.commonprefix(input.index).rstrip(".")
        sam = output.bam.replace('.bam', '.sam')

        shell(
            "bowtie2 "
            "-x {prefix} "
            "-U {input.fastq} "
            "--threads {threads} "
            "-S {sam} "
            "{params.extra} "
            "> {log} 2>&1"
        )

        shell(
            "samtools view -Sb {sam} "
            "| samtools sort - -o {output.bam} -O BAM "
            "&& rm {sam}"
        )


rule fastq_count:
    input:
        fastq='{sample_dir}/{sample}/{sample}{suffix}.fastq.gz'
    output:
        '{sample_dir}/{sample}/{sample}{suffix}.fastq.gz.libsize'
    threads: 1
    resources:
        mem_mb=gb(1),
        runtime=autobump(hours=2)
    shell:
        'zcat {input} | echo $((`wc -l`/4)) > {output}'


rule bam_count:
    input:
        bam='{sample_dir}/{sample}/{suffix}.bam'
    output:
        '{sample_dir}/{sample}/{suffix}.bam.libsize'
    threads: 1
    resources:
        mem_mb=gb(2),
        runtime=autobump(hours=2)
    shell:
        'samtools view -c {input} > {output}'


rule bam_index:
    input:
        bam='{prefix}.bam'
    output:
        bai='{prefix}.bam.bai'
    threads: 1
    resources:
        mem_mb=gb(2),
        runtime=autobump(hours=2)
    shell:
        'samtools index {input} {output}'


rule featurecounts:
    """
    Count reads in annotations with featureCounts from the subread package
    """
    input:
        annotation=rules.gtf.output,
        bam=c.targets['markduplicates']['bam']
    output:
        counts='{sample_dir}/rnaseq_aggregation/featurecounts.txt'
    log:
        '{sample_dir}/rnaseq_aggregation/featurecounts.txt.log'
    threads: 8
    resources:
        mem_mb=gb(16),
        runtime=autobump(hours=2)
    params:
        strand_arg={
                'unstranded': '-s0 ',
                'fr-firststrand': '-s2 ',
                'fr-secondstrand': '-s1 ',
            }[config["stranded"]],
        extra=""
    run:
        # NOTE: By default, we use -p for paired-end
        p_arg = ''
        if c.is_paired:
            p_arg = '-p --countReadPairs '
        shell(
            'featureCounts '
            '{params.strand_arg} '
            '{p_arg} '
            '-T {threads} '
            '-a {input.annotation} '
            '-o {output.counts} '
            '{input.bam} '
            '&> {log}'
        )


rule rrna_libsizes_table:
    """
    Aggregate rRNA counts into a table
    """
    input:
        rrna=c.targets['rrna']['libsize'],
        fastq=c.targets['libsizes']['cutadapt']
    output:
        json=c.patterns['rrna_percentages_yaml'],
        tsv=c.patterns['rrna_percentages_table']
    threads: 1
    resources:
        mem_mb=gb(2),
        runtime=autobump(hours=2)
    run:
        def rrna_sample(f):
            return extract_wildcards(c.patterns['rrna']['libsize'], f)['sample']

        def sample(f):
            return extract_wildcards(c.patterns['libsizes']['cutadapt'], f)['sample']

        def million(f):
            return float(open(f).read()) / 1e6

        rrna = sorted(input.rrna, key=rrna_sample)
        fastq = sorted(input.fastq, key=sample)
        samples = list(map(rrna_sample, rrna))
        rrna_m = list(map(million, rrna))
        fastq_m = list(map(million, fastq))

        df = pd.DataFrame(dict(
            sample=samples,
            million_reads_rRNA=rrna_m,
            million_reads_fastq=fastq_m,
        ))
        df = df.set_index('sample')
        df['rRNA_percentage'] = df.million_reads_rRNA / df.million_reads_fastq * 100

        df[['million_reads_fastq', 'million_reads_rRNA', 'rRNA_percentage']].to_csv(output.tsv, sep='\t')
        y = {
            'id': 'rrna_percentages_table',
            'section_name': 'rRNA content',
            'description': 'Amount of reads mapping to rRNA sequence',
            'plot_type': 'table',
            'pconfig': {
                'id': 'rrna_percentages_table_table',
                'title': 'rRNA content table',
                'min': 0
            },
            'data': yaml.load(df.transpose().to_json(), Loader=yaml.FullLoader),
        }
        with open(output.json, 'w') as fout:
            yaml.dump(y, fout, default_flow_style=False)


rule multiqc:
    input:
        files=(
            expand('data/rnaseq_samples/{sample}/{sample}_R{n}.fastq.gz', sample=SAMPLES, n=n),
            expand(
            'data/rnaseq_samples/{sample}/fastqc/{sample}_R1{kind}.fastq.gz_fastqc.zip',
            sample=SAMPLES, kind=["", ".cutadapt", ".cutadapt.bam"]
            ),
            expand(
                'data/rnaseq_samples/{sample}/{sample}.cutadapt.markdups{ext}',
                sample=SAMPLES, ext=['.bam', '.bam.bai']
            ),
            expand(
                'data/rnaseq_samples/{sample}/{sample}.salmon/quant.sf',
                sample=SAMPLES
            ),
            expand('data/rnaseq_samples/{sample}/{sample}.kallisto/abundance.h5', sample=SAMPLES),
            expand('data/rnaseq_samples/{sample}/{sample}_preseq_c_curve.txt', sample=SAMPLES),
            expand('data/rnaseq_samples/{sample}/rseqc/{sample}_infer_experiment.txt', sample=SAMPLES),
            expand('data/rnaseq_samples/{sample}/rseqc/{sample}_read_distribution.txt', sample=SAMPLES),
            expand('data/rnaseq_samples/{sample}/{sample}.collectrnaseqmetrics.metrics', sample=SAMPLES),
            expand('data/rnaseq_samples/{sample}/{sample}.cutadapt.bam.pos.bigwig', sample=SAMPLES, dir=["pos", "neg"]),
            expand('data/rnaseq_samples/{sample}/idxstat_{sample}.txt', sample=SAMPLES),
            expand('data/rnaseq_samples/{sample}/{sample}.cutadapt.markdups.bam.flagstat', sample=SAMPLES),
            expand('data/rnaseq_samples/{sample}/{sample}.cutadapt.markdups.bam.stats', sample=SAMPLES),
            'data/rnaseq_aggregation/rrna_percentages_table.tsv',
            'data/rnaseq_aggregation/featurecounts.txt',
        ),
        config='config/multiqc_config.yaml'
    output:
        'data/rnaseq_aggregation/multiqc.html'
    log:
        'data/rnaseq_aggregation/multiqc.log'
    threads: 1
    resources:
        mem_mb=gb(2),
        runtime=autobump(hours=2)
    run:
        analysis_directory = set([os.path.dirname(i) for i in input])
        outdir = os.path.dirname(c.targets['multiqc'][0])
        basename = os.path.basename(c.targets['multiqc'][0])
        shell(
            'LC_ALL=en_US.utf8 LC_LANG=en_US.utf8 '
            'multiqc '
            '--quiet '
            '--outdir {outdir} '
            '--force '
            '--filename {basename} '
            '--config {input.config} '
            '{analysis_directory} '
            '&> {log} '
        )


rule markduplicates:
    """
    Mark or remove PCR duplicates with Picard MarkDuplicates
    """
    input:
        bam=c.patterns['bam']
    output:
        bam=c.patterns['markduplicates']['bam'],
        metrics=c.patterns['markduplicates']['metrics']
    log:
        c.patterns['markduplicates']['bam'] + '.log'
    params:
        # NOTE: Be careful with the memory here; make sure you have enough
        # and/or it matches the resources you're requesting in the cluster
        # config.
        java_args='-Xmx20g'
        # java_args='-Xmx2g'  # [TEST SETTINGS -1]
    threads: 1
    resources:
        mem_mb=gb(32),
        runtime=autobump(hours=2),
        disk_mb=autobump(gb=100),
    shell:
        'picard '
        '{params.java_args} '
        'MarkDuplicates '
        'INPUT={input.bam} '
        'OUTPUT={output.bam} '
        'METRICS_FILE={output.metrics} '
        'VALIDATION_STRINGENCY=LENIENT '
        '&> {log}'


rule collectrnaseqmetrics:
    """
    Calculate various RNA-seq QC metrics with Picarc CollectRnaSeqMetrics
    """
    input:
        bam=c.patterns['markduplicates']['bam'],
        refflat=rules.conversion_refflat.output,
    output:
        metrics=c.patterns['collectrnaseqmetrics']['metrics'],
    params:
        # NOTE: Be careful with the memory here; make sure you have enough
        # and/or it matches the resources you're requesting in the cluster
        # config.
        java_args='-Xmx20g',
        # java_args='-Xmx2g',  # [TEST SETTINGS -1]
        strand_arg={
                'unstranded': 'STRAND=NONE ',
                'fr-firststrand': 'STRAND=SECOND_READ_TRANSCRIPTION_STRAND ',
                'fr-secondstrand': 'STRAND=FIRST_READ_TRANSCRIPTION_STRAND ',
            }[config["stranded"]
    log:
        c.patterns['collectrnaseqmetrics']['metrics'] + '.log'
    threads: 1
    resources:
        mem_mb=gb(32),
        runtime=autobump(hours=2)
    run:
        shell(
            'picard '
            '{params.java_args} '
            'CollectRnaSeqMetrics '
            '{params.strand_arg} '
            'VALIDATION_STRINGENCY=LENIENT '
            'REF_FLAT={input.refflat} '
            'INPUT={input.bam} '
            'OUTPUT={output.metrics} '
            '&> {log}'
        )


rule preseq:
    """
    Compute a library complexity curve with preseq
    """
    input:
        bam=c.patterns['bam']
    output:
        c.patterns['preseq']
    threads: 1
    resources:
        mem_mb=gb(1),
        runtime=autobump(hours=2)
    shell:
        'preseq '
        'c_curve '
        '-B {input} '
        '-o {output} '


rule salmon:
    """
    Quantify reads coming from transcripts with Salmon
    """
    input:
        fastq=fill_r1_r2(c.sampletable, c.patterns['cutadapt']),
        index=REFERENCES + "/salmon/versionInfo.json"
    output:
        c.patterns['salmon']
    params:
        index_dir=os.path.dirname(REFERENCES + "/salmon/versionInfo.json"),
        outdir=os.path.dirname(c.patterns['salmon'])
    log:
        c.patterns['salmon'] + '.log'
    threads: 6
    resources:
        mem_mb=gb(32),
        runtime=autobump(hours=2)
    run:
        if c.is_paired:
            fastq_arg = f'-1 {input.fastq[0]} -2 {input.fastq[1]} '
        else:
            fastq_arg = f'-r {input.fastq} '
        shell(
            'salmon quant '
            '--index {params.index_dir} '
            '--output {params.outdir} '
            '--threads {threads} '

            # NOTE: --libType=A auto-detects library type. Change if needed.
            '--libType=A '

            # NOTE: Docs suggest using --gcBias, --validateMappings, and
            # --seqBias is a good idea
            '--gcBias '
            '--seqBias '
            '--validateMappings '
            '{fastq_arg} '
            '&> {log}'
        )


rule kallisto:
    """
    Quantify reads coming from transcripts with Kallisto
    """
    input:
        fastq=fill_r1_r2(c.sampletable, c.patterns['cutadapt']),
        index=REFERENCES + "/kallisto/transcripts.idx",
    output:
        c.patterns['kallisto']
    params:
        index_dir=os.path.dirname(REFERENCES + "/kallisto/transcripts.idx"),
        outdir=os.path.dirname(c.patterns['kallisto']),
        strand_arg={
                'unstranded': '',
                'fr-firststrand': '--rf-stranded',
                'fr-secondstrand': '--fr-stranded',
            }[config["stranded"]
    log:
        c.patterns['kallisto'] + '.log'
    threads:
        8
    resources:
        mem_mb=gb(32),
        runtime=autobump(hours=2),
    run:
        if c.is_paired:
            se_args = ''
            assert len(input.fastq) == 2
        else:
            # For single-end, add the experimentally-determined fragment length
            # and standard deviation here
            se_args = '--single --fragment-length 300 --sd 20 '
            assert len(input.fastq) == 1
        shell(
            'kallisto quant '
            '--index {input.index} '
            '--output-dir {params.outdir} '
            '--threads {threads} '
            '--bootstrap-samples 100 '
            '--threads {threads} '
            '{se_args} '
            '{params.strand_arg} '
            '{input.fastq} '
            '&> {log}'
        )

rule rseqc_infer_experiment:
    """
    Infer strandedness of experiment
    """
    input:
        bam=c.patterns['markduplicates']['bam'],
        bed12=rules.conversion_bed12.output,
    output:
        txt=c.patterns['rseqc']['infer_experiment']
    log:
        c.patterns['rseqc']['infer_experiment'] + '.log'
    resources:
        mem_mb=gb(2),
        runtime=autobump(hours=2)

    shell:
        'infer_experiment.py -r {input.bed12} -i {input.bam} > {output} &> {log}'

rule rseqc_read_distribution:
    """
    read distribution plots
    """
    input:
        bam=c.patterns['markduplicates']['bam'],
        bed12=rules.conversion_bed12.output,
    output:
        txt=c.patterns['rseqc']['read_distribution']
    log:
        c.patterns['rseqc']['read_distribution'] + '.log'
    resources:
        mem_mb=gb(2),
        runtime=autobump(hours=2)
    shell:
        'read_distribution.py -i {input.bam} -r {input.bed12} > {output} &> {log}'


rule idxstats:
    """
    Run samtools idxstats on sample bams
    """
    input:
        bam=c.patterns['markduplicates']['bam'],
        bai=c.patterns['markduplicates']['bam'] + '.bai'
    output:
        txt=c.patterns['samtools']['idxstats']
    log: 
        c.patterns['samtools']['idxstats'] + '.log'
    resources:
        mem_mb=gb(16),
        runtime=autobump(hours=2)
    run:
        shell(
            'samtools idxstats {input.bam} 2> {log} 1> {output.txt}'
        )


# Common arguments used for bamCoverage rules below
BAMCOVERAGE_ARGS = (
    '--minMappingQuality 20 '  # excludes multimappers
    '--smoothLength 10 '       # smooth signal with specified window
    # '--normalizeUsing BPM '    # equivalent to TPM # [TEST SETTINGS]
)

rule bigwig_neg:
    """
    Create a bigwig for negative-strand reads
    """
    input:
        bam=c.patterns['markduplicates']['bam'],
        bai=c.patterns['markduplicates']['bam'] + '.bai',
    output: c.patterns['bigwig']['neg']
    threads: 8
    resources:
        mem_mb=gb(16),
        runtime=autobump(hours=2)
    log:
        c.patterns['bigwig']['neg'] + '.log'
    params:
        strand_arg = {
                'unstranded': '',
                'fr-firststrand': '--filterRNAstrand reverse ',
                'fr-secondstrand': '--filterRNAstrand forward ',
            }[config["stranded"]
    run:
        shell(
            'bamCoverage '
            '--bam {input.bam} '
            '-o {output} '
            '-p {threads} '
            '{BAMCOVERAGE_ARGS} '
            '{params.strand_arg} '
            '&> {log}'
        )


rule bigwig_pos:
    """
    Create a bigwig for postive-strand reads.
    """
    input:
        bam=c.patterns['markduplicates']['bam'],
        bai=c.patterns['markduplicates']['bam'] + '.bai',
    output: c.patterns['bigwig']['pos']
    threads: 8
    resources:
        mem_mb=gb(16),
        runtime=autobump(hours=2)
    log:
        c.patterns['bigwig']['pos'] + '.log'
    params:
        strand_arg={
                'unstranded': '',
                'fr-firststrand': '--filterRNAstrand forward ',
                'fr-secondstrand': '--filterRNAstrand reverse ',
            }[config["stranded"]
    run:
        shell(
            'bamCoverage '
            '--bam {input.bam} '
            '-o {output} '
            '-p {threads} '
            '{BAMCOVERAGE_ARGS} '
            '{params.strand_arg} '
            '&> {log}'
        )


def bigwigs_to_merge(wc):
    chunk = config['merged_bigwigs'][wc.merged_bigwig_label]
    neg_labels = chunk.get('neg', [])
    pos_labels = chunk.get('pos', [])
    pos_bigwigs = expand(
        c.patterns['bigwig']['pos'],
        sample=pos_labels
    )
    neg_bigwigs = expand(
        c.patterns['bigwig']['neg'],
        sample=neg_labels)
    return pos_bigwigs + neg_bigwigs


rule flagstat:
    input:
        bam=c.patterns['markduplicates']['bam'],
        bai=c.patterns['markduplicates']['bam'] + '.bai'
    output:
        c.patterns['samtools']['flagstat']
    log:
        c.patterns['samtools']['flagstat'] + '.log'
    shell:
        'samtools flagstat {input.bam} > {output}'


rule samtools_stats:
    input:
        bam=c.patterns['markduplicates']['bam'],
        bai=c.patterns['markduplicates']['bam'] + '.bai'
    output:
        c.patterns['samtools']['stats']
    log:
        c.patterns['samtools']['stats'] + '.log'
    shell:
        'samtools stats {input.bam} > {output}'



# vim: ft=python
