import sys
sys.path.insert(0, srcdir('../..'))
import os
from textwrap import dedent
import yaml
import tempfile
import pandas as pd
from lcdblib.snakemake import helpers, aligners
from lcdblib.utils import utils
from lib import common
from lib import cluster_specific
from lib.patterns_targets import RNASeqConfig

# ----------------------------------------------------------------------------
#
# Search for the string "NOTE:" to look for points of configuration that might
# be helpful for your experiment.
#
# ----------------------------------------------------------------------------

# Only use default if no configfile specified on the command line with
# --configfile
if not workflow.overwrite_configfile:
    configfile: 'config/config.yaml'
else:
    configfile: workflow.overwrite_configfile

include: '../references/Snakefile'
shell.prefix('set -euo pipefail; export TMPDIR={};'.format(cluster_specific.tempdir_for_biowulf()))
shell.executable('/bin/bash')

config = common.load_config(config)

c = RNASeqConfig(config, config.get('patterns', 'config/rnaseq_patterns.yaml'))

wildcard_constraints:
    n = '[1,2]'


def wrapper_for(path):
    return 'file:' + os.path.join('../..','wrappers', 'wrappers', path)

# ----------------------------------------------------------------------------
# RULES
# ----------------------------------------------------------------------------

# See "patterns and targets" in the documentation for what's going on here.
final_targets = utils.flatten((
    c.targets['bam'],
    utils.flatten(c.targets['fastqc']),
    utils.flatten(c.targets['libsizes']),
    [c.targets['fastq_screen']],
    [c.targets['libsizes_table']],
    [c.targets['rrna_percentages_table']],
    [c.targets['multiqc']],
    utils.flatten(c.targets['featurecounts']),
    utils.flatten(c.targets['rrna']),
    utils.flatten(c.targets['markduplicates']),
    utils.flatten(c.targets['salmon']),
    utils.flatten(c.targets['dupradar']),
    utils.flatten(c.targets['preseq']),
    utils.flatten(c.targets['rseqc']),
    utils.flatten(c.targets['collectrnaseqmetrics']),
    utils.flatten(c.targets['bigwig']),
    utils.flatten(c.targets['downstream']),
))

if 'merged_bigwigs' in config:
    final_targets.extend(utils.flatten(c.targets['merged_bigwig']))


rule targets:
    """
    Final targets to create
    """
    input: final_targets

if 'orig_filename' in c.sampletable.columns:

    # Convert the sampletable to be indexed by the first column, for
    # convenience in generating the input/output filenames.
    _st = c.sampletable.set_index(c.sampletable.columns[0])

    def orig_for_sample(wc):
        """
        Given a sample, returns either one or two original fastq files
        depending on whether the library was single- or paired-end.
        """
        row = _st.loc[wc.sample]
        res = [row['orig_filename']]
        try:
            r2 = row['orig_filename_R2']
            if isinstance(r2, str):
                res.append(row['orig_filename_R2'])
        except KeyError:
            pass
        return res

    rule symlinks:
        """
        Symlinks files over from original filename
        """
        input:
            orig_for_sample
        output:
            c.patterns['fastq']
        wildcard_constraints:
            n="\d+"
        run:
            for src, linkname in zip(input, output):
                utils.make_relative_symlink(src, linkname)


    rule symlink_targets:
        input: c.targets['fastq']

if 'Run' in c.sampletable.columns and sum(c.sampletable['Run'].str.startswith('SRR')) > 0:

    # Convert the sampletable to be indexed by the first column, for
    # convenience in generating the input/output filenames.
    _st = c.sampletable.set_index(c.sampletable.columns[0])

    rule fastq_dump:
        output:
            c.patterns['fastq']
        run:
            srr = _st.loc[wildcards.sample, 'Run']

            # Two different paths depending on the layout. In both cases, we
            # want to avoid creating the final output until the very end, to
            # avoid incomplete downloads.
            if common.is_paired_end(c.sampletable, wildcards.sample):

                # For PE we need to use --split-files, which also means using
                # the slower --gzip
                shell(
                    'fastq-dump '
                    '{srr} '
                    '--gzip '
                    '--split-files '
                    # '-X 100000 ' # [TEST SETTINGS]
                )

                # The filenames are predictable, so we can move them as needd.
                shell(
                    'mv {srr}_1.fastq.gz '
                    '$(dirname {output})/{wildcards.sample}_R1.fastq.gz'
                )
                shell(
                    'mv {srr}_2.fastq.gz '
                    '$(dirname {output})/{wildcards.sample}_R2.fastq.gz'
                )

            else:
                # For SE, we can use the faster stdout | gzip, and move it
                # directly when done.
                shell(
                    'fastq-dump '
                    '{srr} '
                    '-Z '
                    # '-X 100000 ' # [TEST SETTINGS]
                    '| gzip -c > {output}.tmp '
                    '&& mv {output}.tmp {output} '
                )


def render_r1_r2(pattern):
    return expand(pattern, sample='{sample}', n=[1,2])

rule cutadapt:
    """
    Run cutadapt
    """
    input:
        fastq=common.fill_r1_r2(c.sampletable, c.patterns['fastq'])
    output:
        fastq=render_r1_r2(c.patterns['cutadapt'])
    log:
        render_r1_r2(c.patterns['cutadapt'])[0] + '.log'
    run:
        paired = len(input) == 2

        # NOTE: Change cutadapt params here
        extra='-a file:../../include/adapters.fa -q 20 --minimum-length=25'

        if paired:
            shell(
                "cutadapt "
                "{extra} "
                "{input.fastq[0]} "
                "{input.fastq[1]} "
                "-o {output[0]} "
                "-p {output[1]} "
                "&> {log}"
            )
        else:
            shell(
                "cutadapt "
                "{extra} "
                "{input.fastq[0]} "
                "-o {output[0]} "
                "&> {log}"
            )
            shell('touch {output[1]}')


rule fastqc:
    """
    Run FastQC
    """
    input: '{sample_dir}/{sample}/{sample}{suffix}'
    output:
        html='{sample_dir}/{sample}/fastqc/{sample}{suffix}_fastqc.html',
        zip='{sample_dir}/{sample}/fastqc/{sample}{suffix}_fastqc.zip',
    wrapper:
        wrapper_for('fastqc')


if config['aligner']['index'] == 'hisat2':
    rule hisat2:
        """
        Map reads with HISAT2
        """
        input:
            fastq=common.fill_r1_r2(c.sampletable, c.patterns['cutadapt']),
            index=[c.refdict[c.organism][config['aligner']['tag']]['hisat2']]
        output:
            bam=c.patterns['bam']
        log:
            c.patterns['bam'] + '.log'
        params:
            # NOTE: see examples at
            # https://github.com/lcdb/lcdb-wf/tree/master/wrappers/wrappers/hisat2/align
            # for details on setting hisat2 params and samtools params separately.
            samtools_view_extra='-F 0x04'
        threads: 6
        wrapper:
            wrapper_for('hisat2/align')

if config['aligner']['index'] == 'star':
    rule star:
        """
        Map reads with STAR
        """
        input:
            fastq=common.fill_r1_r2(c.sampletable, c.patterns['cutadapt']),
            index=[c.refdict[c.organism][config['aligner']['tag']]['star']]
        output:
            bam=c.patterns['bam']
        log:
            c.patterns['bam'] + '.log'
        threads: 6
        run:
            genomedir = os.path.dirname(input.index[0])
            prefix = os.path.dirname(output[0]) + '.star.'
            shell(
                'STAR '
                '--runThreadN {threads} '
                '--genomeDir {genomedir} '
                '--readFilesIn {input.fastq} '
                '--readFilesCommand zcat '
                '--outFileNamePrefix {prefix} '
                # NOTE: The STAR docs indicate that the following parameters
                # are standard options for ENCODE long-RNA-seq pipeline.
                # Comments are from the STAR docs.

                # reduces number of spurious junctions
                '--outFilterType BySJout '

                # if more than this many multimappers, consider unmapped
                '--outFilterMultimapNmax 20 '

                # min overhang for unannotated junctions
                '--alignSJoverhangMin 8 '

                # min overhang for annotated junctions
                '--alignSJDBoverhangMin 1 '

                # max mismatches per pair
                '--outFilterMismatchNmax 999 '

                # max mismatches per pair relative to read length
                '--outFilterMismatchNoverReadLmax 0.04 '

                # min intron length
                '--alignIntronMin 20 '

                # max intron length
                '--alignIntronMax 1000000 '

                # max distance between mates
                '--alignMatesGapMax 1000000 '

                 '&> {log} '
            )

            shell(
                'samtools view '
                '-Sb '
                '-F 0x04 '
                '{prefix}Aligned.out.sam '
                '> {output}.tmp.bam '
                '&& rm {prefix}Aligned.out.sam '
            )
            shell(
                "samtools sort "
                "-o {output.bam} "
                # NOTE: optionally provide other sort arguments here. If you
                # increase threads, you may need to increase memory in the
                # clusterconfig as well.
                "-O BAM "
                "{output}.tmp.bam "
                "&& rm {output}.tmp.bam "
            )

if config['aligner']['index'] == 'ngm':
    rule ngm:
        """
        Map reads with NextGenMap
        """
        input:
            fastq=common.fill_r1_r2(c.sampletable, c.patterns['cutadapt']),
            index=[c.refdict[c.organism][config['aligner']['tag']]['ngm']],
            fasta=[c.refdict[c.organism][config['aligner']['tag']]['ngm_fasta']]
        output:
            bam=c.patterns['bam']
        log:
            c.patterns['bam'] + '.log'
        threads: 6
        run:
            if common.is_paired_end(c.sampletable, wildcards.sample):
                fastqs = '-1 {0} -2 {1} '.format(*input.fastq)
            else:
                fastqs = '--qry {0} '.format(input.fastq)

            prefix = input.index[0].replace('-enc.2.ngm', '')

            output_prefix = output.bam.replace('.bam', '')

            shell(
                "ngm "
                "-r {prefix} "
                "{fastqs} "
                "-t {threads} "
                "-o {output_prefix}.sam "
                # NOTE: add extra NextGenMap options here.
                "&> {log}"
            )

            shell(
                "samtools view -Sb "
                # NOTE: change samtools view arguments here if needed; default
                # is to remove unmapped reads
                "-F 0x04 "
                "{output_prefix}.sam "
                "> {output_prefix}.tmp.bam && rm {output_prefix}.sam"
            )

            shell(
                "samtools sort "
                "-o {output.bam} "
                # NOTE: optionally provide other sort arguments here. If you
                # increase threads, you may need to increase memory in the
                # clusterconfig as well.
                "-O BAM "
                "{output_prefix}.tmp.bam "
                "&& rm {output_prefix}.tmp.bam "
            )


rule rRNA:
    """
    Map reads with bowtie2 to the rRNA reference
    """
    input:
        fastq=common.fill_r1_r2(c.sampletable, c.patterns['cutadapt'], r1_only=True),
        index=[c.refdict[c.organism][config['rrna']['tag']]['bowtie2']]
    output:
        bam=c.patterns['rrna']['bam']
    log:
        c.patterns['rrna']['bam'] + '.log'
    params:
        # NOTE: we'd likely only want to report a single alignment for rRNA
        # screening
        bowtie2_extra='-k 1',
        samtools_view_extra='-F 0x04'
    threads: 6
    wrapper:
        wrapper_for('bowtie2/align')


rule fastq_count:
    """
    Count reads in a FASTQ file
    """
    input:
        fastq='{sample_dir}/{sample}/{sample}{suffix}.fastq.gz'
    output:
        count='{sample_dir}/{sample}/{sample}{suffix}.fastq.gz.libsize'
    shell:
        'zcat {input} | echo $((`wc -l`/4)) > {output}'


rule bam_count:
    """
    Count reads in a BAM file
    """
    input:
        bam='{sample_dir}/{sample}/{suffix}.bam'
    output:
        count='{sample_dir}/{sample}/{suffix}.bam.libsize'
    shell:
        'samtools view -c {input} > {output}'


rule bam_index:
    """
    Index a BAM
    """
    input:
        bam='{prefix}.bam'
    output:
        bai='{prefix}.bam.bai'
    shell:
        'samtools index {input} {output}'


def fastq_screen_references():
    """
    Returns the Bowtie2 indexes for the configured references from the
    `fastq_screen:` section of the config
    """
    refs = {}
    for i in config['fastq_screen']:
        refs[i['label']] = c.refdict[i['organism']][i['tag']]['bowtie2']
    return refs


rule fastq_screen:
    """
    Run fastq_screen to look for contamination from other genomes
    """
    input:
        **fastq_screen_references(),
        fastq=common.fill_r1_r2(c.sampletable, rules.cutadapt.output.fastq, r1_only=True),
    output:
        txt=c.patterns['fastq_screen']
    log:
        c.patterns['fastq_screen'] + '.log'
    params: subset=100000
    wrapper:
        wrapper_for('fastq_screen')


rule featurecounts:
    """
    Count reads in annotations with featureCounts from the subread package
    """
    input:
        annotation=c.refdict[c.organism][config['gtf']['tag']]['gtf'],
        bam=c.patterns['bam']
    output:
        counts='{sample_dir}/{sample}/{sample}.cutadapt.bam.featurecounts.{stranded}.txt'
    log:
        '{sample_dir}/{sample}/{sample}.cutadapt.bam.featurecounts.{stranded}.txt.log'
    shell:
        'featureCounts '

        # NOTE:
        # By default, this rule runs three times, using a different strand
        # setting each time. The strand argument (-s0, -s1, -s2) to
        # featureCounts is pulled straight from the wildcards, and the
        # corresponding filenames have been set up in rnaseq_patterns.yaml.
        #
        # You probably do NOT want to add your own -s argument below, however
        # other arguments might be useful. For example, for nascent RNA-seq,
        # add '-t gene -g gene_id -f '
        '-{wildcards.stranded} '

        # NOTE: additional featureCounts args here
        '-T {threads} '
        '-a {input.annotation} '
        '-o {output.counts} '
        '{input.bam} '
        '&> {log}'


rule rrna_libsizes_table:
    """
    Aggregate rRNA counts into a table
    """
    input:
        rrna=c.targets['rrna']['libsize'],
        fastq=c.targets['libsizes']['cutadapt']
    output:
        json=c.patterns['rrna_percentages_yaml'],
        tsv=c.patterns['rrna_percentages_table']
    run:
        def rrna_sample(f):
            return helpers.extract_wildcards(c.patterns['rrna']['libsize'], f)['sample']

        def sample(f):
            return helpers.extract_wildcards(c.patterns['libsizes']['cutadapt'], f)['sample']

        def million(f):
            return float(open(f).read()) / 1e6

        rrna = sorted(input.rrna, key=rrna_sample)
        fastq = sorted(input.fastq, key=sample)
        samples = list(map(rrna_sample, rrna))
        rrna_m = list(map(million, rrna))
        fastq_m = list(map(million, fastq))

        df = pd.DataFrame(dict(
            sample=samples,
            million_reads_rRNA=rrna_m,
            million_reads_fastq=fastq_m,
        ))
        df = df.set_index('sample')
        df['rRNA_percentage'] = df.million_reads_rRNA / df.million_reads_fastq * 100

        df[['million_reads_fastq', 'million_reads_rRNA', 'rRNA_percentage']].to_csv(output.tsv, sep='\t')
        y = {
            'id': 'rrna_percentages_table',
            'section_name': 'rRNA content',
            'description': 'Amount of reads mapping to rRNA sequence',
            'plot_type': 'table',
            'pconfig': {
                'id': 'rrna_percentages_table_table',
                'title': 'rRNA content table',
                'min': 0
            },
            'data': yaml.load(df.transpose().to_json()),
        }
        with open(output.json, 'w') as fout:
            yaml.dump(y, fout, default_flow_style=False)


rule libsizes_table:
    """
    Aggregate fastq and bam counts in to a single table
    """
    input:
        utils.flatten(c.targets['libsizes'])
    output:
        json=c.patterns['libsizes_yaml'],
        tsv=c.patterns['libsizes_table']
    run:
        def sample(f):
            return os.path.basename(os.path.dirname(f))

        def million(f):
            return float(open(f).read()) / 1e6

        def stage(f):
            return os.path.basename(f).split('.', 1)[1].replace('.gz', '').replace('.count', '')

        df = pd.DataFrame(dict(filename=list(map(str, input))))
        df['sample'] = df.filename.apply(sample)
        df['million'] = df.filename.apply(million)
        df['stage'] = df.filename.apply(stage)
        df = df.set_index('filename')
        df = df.pivot('sample', columns='stage', values='million')

        # make nicer column names
        convert = {
            'fastq.libsize': 'stage1_raw',
            'cutadapt.fastq.libsize' : 'stage2_trimmed',
            'cutadapt.bam.libsize': 'stage3_aligned',
        }

        df.columns = [convert[i] for i in df.columns]

        df.to_csv(output.tsv, sep='\t')
        y = {
            'id': 'libsizes_table',
            'section_name': 'Library sizes',
            'description': 'Library sizes at various stages of the pipeline',
            'plot_type': 'table',
            'pconfig': {
                'id': 'libsizes_table_table',
                'title': 'Library size table',
                'min': 0
            },
            'data': yaml.load(df.transpose().to_json()),
        }
        with open(output.json, 'w') as fout:
            yaml.dump(y, fout, default_flow_style=False)


rule multiqc:
    """
    Aggregate various QC stats and logs into a single HTML report with MultiQC
    """
    # NOTE: if you add more rules and want MultiQC to pick up the output, then
    # add outputs from those rules to the inputs here.
    input:
        files=(
            utils.flatten(c.targets['fastqc']) +
            utils.flatten(c.targets['libsizes_yaml']) +
            utils.flatten(c.targets['rrna_percentages_yaml']) +
            utils.flatten(c.targets['cutadapt']) +
            utils.flatten(c.targets['featurecounts']) +
            utils.flatten(c.targets['bam']) +
            utils.flatten(c.targets['markduplicates']) +
            utils.flatten(c.targets['salmon']) +
            utils.flatten(c.targets['rseqc']) +
            utils.flatten(c.targets['fastq_screen']) +
            utils.flatten(c.targets['dupradar']) +
            utils.flatten(c.targets['preseq']) +
            utils.flatten(c.targets['collectrnaseqmetrics'])
        ),
        config='config/multiqc_config.yaml'
    output: c.targets['multiqc']
    log: c.targets['multiqc'][0] + '.log'
    run:
        analysis_directory = set([os.path.dirname(i) for i in input])
        outdir = os.path.dirname(c.targets['multiqc'][0])
        basename = os.path.basename(c.targets['multiqc'][0])
        shell(
            'LC_ALL=en_US.UTF.8 LC_LANG=en_US.UTF-8 '
            'multiqc '
            '--quiet '
            '--outdir {outdir} '
            '--force '
            '--filename {basename} '
            '--config {input.config} '
            '{analysis_directory} '
            '&> {log} '
        )

rule markduplicates:
    """
    Mark or remove PCR duplicates with Picard MarkDuplicates
    """
    input:
        bam=c.patterns['bam']
    output:
        bam=c.patterns['markduplicates']['bam'],
        metrics=c.patterns['markduplicates']['metrics']
    log:
        c.patterns['markduplicates']['bam'] + '.log'
    params:
        # NOTE: Be careful with the memory here; make sure you have enough
        # and/or it matches the resources you're requesting in the cluster
        # config.
        java_args='-Xmx20g'
        # java_args='-Xmx2g'  # [TEST SETTINGS -1]
    shell:
        'picard '
        '{params.java_args} '
        'MarkDuplicates '
        'INPUT={input.bam} '
        'OUTPUT={output.bam} '
        'METRICS_FILE={output.metrics} '
        '&> {log}'


rule collectrnaseqmetrics:
    """
    Calculate various RNA-seq QC metrics with Picarc CollectRnaSeqMetrics
    """
    input:
        bam=c.patterns['bam'],
        refflat=c.refdict[c.organism][config['gtf']['tag']]['refflat']
    output:
        metrics=c.patterns['collectrnaseqmetrics']['metrics'],
        pdf=c.patterns['collectrnaseqmetrics']['pdf']
    params:
        # NOTE: Be careful with the memory here; make sure you have enough
        # and/or it matches the resources you're requesting in the cluster
        # config.
        java_args='-Xmx20g'
        # java_args='-Xmx2g'  # [TEST SETTINGS -1]
    log:
        c.patterns['collectrnaseqmetrics']['metrics'] + '.log'
    shell:
        'picard '
        '{params.java_args} '
        'CollectRnaSeqMetrics '

        # NOTE: Adjust strandedness appropriately. From the Picard docs:
        #
        #     STRAND=StrandSpecificity For strand-specific library prep. For
        #     unpaired reads, use FIRST_READ_TRANSCRIPTION_STRAND if the reads
        #     are expected to be on the transcription strand.  Required.
        #     Possible values: {NONE, FIRST_READ_TRANSCRIPTION_STRAND,
        #     SECOND_READ_TRANSCRIPTION_STRAND}
        #
        'STRAND=NONE '

        'CHART_OUTPUT={output.pdf} '
        'REF_FLAT={input.refflat} '
        'INPUT={input.bam} '
        'OUTPUT={output.metrics} '
        '&> {log}'


rule preseq:
    """
    Compute a library complexity curve with preseq
    """
    input:
        bam=c.patterns['bam']
    output:
        c.patterns['preseq']
    shell:
        'preseq '
        'c_curve '
        '-B {input} '
        '-o {output} '


rule dupRadar:
    """
    Assess the library complexity with dupRadar
    """
    input:
        bam=rules.markduplicates.output.bam,
        annotation=c.refdict[c.organism][config['gtf']['tag']]['gtf'],
    output:
        density_scatter=c.patterns['dupradar']['density_scatter'],
        expression_histogram=c.patterns['dupradar']['expression_histogram'],
        expression_boxplot=c.patterns['dupradar']['expression_boxplot'],
        expression_barplot=c.patterns['dupradar']['expression_barplot'],
        multimapping_histogram=c.patterns['dupradar']['multimapping_histogram'],
        dataframe=c.patterns['dupradar']['dataframe'],
        model=c.patterns['dupradar']['model'],
        curve=c.patterns['dupradar']['curve'],
    log: c.patterns['dupradar']['dataframe'] + '.log'
    wrapper:
        wrapper_for('dupradar')


rule salmon:
    """
    Quantify reads coming from transcripts with Salmon
    """
    input:
        fastq=common.fill_r1_r2(c.sampletable, c.patterns['cutadapt']),
        index=c.refdict[c.organism][config['salmon']['tag']]['salmon'],
    output:
        c.patterns['salmon']
    params:
        index_dir=os.path.dirname(c.refdict[c.organism][config['salmon']['tag']]['salmon']),
        outdir=os.path.dirname(c.patterns['salmon'])
    log:
        c.patterns['salmon'] + '.log'
    run:
        paired = len(input.fastq) == 2
        if paired:
            shell(
                # NOTE: adjust Salmon params as needed
                'salmon quant '
                '--index {params.index_dir} '
                '--output {params.outdir} '
                '--threads {threads} '

                # NOTE: --libType=A auto-detects library type. Change if needed.
                '--libType=A '

                # NOTE: Docs suggest using --gcBias and --seqBias is a good idea
                '--gcBias '
                '--seqBias '
                '-1 {input.fastq[0]} '
                '-2 {input.fastq[1]} '
                '&> {log}'
            )
        else:
            shell(
                # NOTE: adjust Salmon params as needed
                'salmon quant '
                '--index {params.index_dir} '
                '--output {params.outdir} '
                '--threads {threads} '

                # NOTE: --libType=A auto-detects library type. Change if needed.
                '--libType=A '

                # NOTE: Docs suggest using --gcBias and --seqBias is a good idea
                '--gcBias '
                '--seqBias '
                '-r {input.fastq} '
                '&> {log}'
            )


rule rseqc_bam_stat:
    """
    Calculate various BAM stats with RSeQC
    """
    input:
        bam=c.patterns['bam']
    output:
        txt=c.patterns['rseqc']['bam_stat']
    wrapper: wrapper_for('rseqc/bam_stat')


rule bigwig_neg:
    """
    Create a bigwig for negative-strand reads
    """
    input:
        bam=c.patterns['bam'],
        bai=c.patterns['bam'] + '.bai',
    output: c.patterns['bigwig']['neg']
    threads: 8
    log:
        c.patterns['bigwig']['neg'] + '.log'
    shell:
        # NOTE: adjust bamCoverage params as needed
        # Make sure the bigwig_pos rule below reflects the same changes.
        'bamCoverage '
        '--bam {input.bam} '
        '-o {output} '
        '-p {threads} '
        '--minMappingQuality 20 '
        '--ignoreDuplicates '
        '--smoothLength 10 '
        '--filterRNAstrand forward '
        '--normalizeUsing BPM '
        '&> {log}'


rule bigwig_pos:
    """
    Create a bigwig for postive-strand reads.
    """
    input:
        bam=c.patterns['bam'],
        bai=c.patterns['bam'] + '.bai',
    output: c.patterns['bigwig']['pos']
    threads: 8
    log:
        c.patterns['bigwig']['pos'] + '.log'
    shell:
        # NOTE: adjust bamCoverage params as needed
        # Make sure the bigwig_neg rule above reflects the same changes.
        'bamCoverage '
        '--bam {input.bam} '
        '-o {output} '
        '-p {threads} '
        '--minMappingQuality 20 '
        '--ignoreDuplicates '
        '--smoothLength 10 '
        '--filterRNAstrand reverse '
        '--normalizeUsing BPM '
        '&> {log}'


rule symlink_bigwigs:
    input:
        pos=c.patterns['bigwig']['pos'],
        neg=c.patterns['bigwig']['neg'],
    output:
        sense=c.patterns['bigwig']['sense'],
        antisense=c.patterns['bigwig']['antisense'],
    run:
        # NOTE:
        #    In our test data, reads mapping to the positive strand correspond
        #    to sense-strand transcripts. If your protocol is reversed (e.g.,
        #    TruSeq kits), you can map negative-strand reads to "sense". The
        #    track hub creation in rnaseq_trackhub.py only cares about the
        #    `sense` and `antisense` versions, so it's cheap to mess around
        #    with the symlinking here.
        #
        # E.g., use this for TruSeq:
        # utils.make_relative_symlink(input.pos, output.antisense)
        # utils.make_relative_symlink(input.neg, output.sense)

        utils.make_relative_symlink(input.pos, output.sense)
        utils.make_relative_symlink(input.neg, output.antisense)


rule rnaseq_rmarkdown:
    """
    Run and render the RMarkdown file that performs differential expression
    """
    input:
        featurecounts=utils.flatten(c.targets['featurecounts']),
        salmon=utils.flatten(c.targets['salmon']),

        # NOTE: the Rmd will likely need heavy editing depending on the project.
        rmd='downstream/rnaseq.Rmd',
        sampletable=config['sampletable']
    output:
        'downstream/rnaseq.html'
    shell:
        'Rscript -e '
        '''"rmarkdown::render('{input.rmd}', 'knitrBootstrap::bootstrap_document')"'''


def bigwigs_to_merge(wc):
    chunk = config['merged_bigwigs'][wc.merged_bigwig_label]
    antisense_labels = chunk.get('antisense', [])
    sense_labels = chunk.get('sense', [])
    sense_bigwigs = expand(
        c.patterns['bigwig']['sense'],
        sample=sense_labels
    )
    antisense_bigwigs = expand(
        c.patterns['bigwig']['antisense'],
        sample=antisense_labels)
    return sense_bigwigs + antisense_bigwigs

if 'merged_bigwigs' in config:
    rule merge_bigwigs:
        """
        Merge together bigWigs as specified in the config ("merged_bigwigs"
        section).
        """
        input:
            bigwigs=bigwigs_to_merge,
            chromsizes=refdict[c.organism][config['aligner']['tag']]['chromsizes'],
        output:
            c.patterns['merged_bigwig']
        log:
            c.patterns['merged_bigwig'] + '.log'
        wrapper:
            wrapper_for('average-bigwigs')

# vim: ft=python
