import os
import sys
sys.path.insert(0, srcdir('../..'))
import gzip
import yaml
import importlib
import tempfile
import pandas
from snakemake.utils import makedirs
from lib.imports import resolve_name
from lib import utils
from lib import aligners, helpers
from lib import cluster_specific
from lib import common

configfile: 'config/config.yaml'

config = common.load_config(config)

tempfile.tempdir = cluster_specific.tempdir_for_biowulf()

shell.prefix('set -euo pipefail; export TMPDIR={};'.format(cluster_specific.tempdir_for_biowulf()))
shell.executable('/bin/bash')
references_dir = common.get_references_dir(config)
refdict, conversion_kwargs = common.references_dict(config)

makedirs([references_dir, os.path.join(references_dir, 'logs')])


def wrapper_for(path):
    return 'file:' + os.path.join('../..','wrappers', 'wrappers', path)


localrules: symlink_fasta_to_index_dir

wildcard_constraints:
    _type="genome|transcriptome|annotation",
    _ext="fasta|gtf"


rule all_references:
    input: utils.flatten(refdict)


rule download_and_process:
    """Downloads the configured URL, applies any configured post-processing, and
    saves the resulting gzipped file to *.fasta.gz or *.gtf.gz.
    """
    output:
        temporary('{references_dir}/{organism}/{tag}/{_type}/{organism}_{tag}.{_ext}.gz')
    run:
        common.download_and_postprocess(output[0], config, wildcards.organism, wildcards.tag, wildcards._type)


rule unzip:
    """Generic rule to unzip files as needed, for example when building
    indexes.
    """
    input:
        rules.download_and_process.output
    output:
        protected('{references_dir}/{organism}/{tag}/{_type}/{organism}_{tag}.{_ext}')
    wildcard_constraints:
        _type="genome|annotation"
    log:
        '{references_dir}/logs/{organism}/{tag}/{_type}/{organism}_{tag}.{_ext}.log'
    shell: 'gunzip -c {input} > {output}'


rule bowtie2_index:
    """
    Build bowtie2 index
    """
    input:
        '{references_dir}/{organism}/{tag}/genome/{organism}_{tag}.fasta'
    output:
        protected(aligners.bowtie2_index_from_prefix('{references_dir}/{organism}/{tag}/genome/bowtie2/{organism}_{tag}'))
    log:
        '{references_dir}/logs/{organism}/{tag}/genome/bowtie2/{organism}_{tag}.log'
    run:
        prefix = aligners.prefix_from_bowtie2_index(output)
        shell(
            'bowtie2-build '
            '{input} '
            '{prefix} '
            '&> {log}')


rule star_index:
    input:
        fasta='{references_dir}/{organism}/{tag}/genome/{organism}_{tag}.fasta',
        gtf='{references_dir}/{organism}/{tag}/annotation/{organism}_{tag}.gtf',
    output:
        protected('{references_dir}/{organism}/{tag}/genome/star/{organism}_{tag}/Genome')
    log:
        '{references_dir}/{organism}/{tag}/genome/star/{organism}_{tag}/Genome.log'
    threads:
        8
    run:
        genomedir = os.path.dirname(output[0])
        shell('rm -r {genomedir}')
        shell('mkdir -p {genomedir}')
        shell(
            'STAR '
            '--runMode genomeGenerate '
            '--runThreadN {threads} '
            '--genomeDir {genomedir} '
            '--genomeFastaFiles {input.fasta} '

            # NOTE: GTF is optional
            '--sjdbGTFfile {input.gtf} '

            # NOTE: STAR docs say that 100 should work well.
            '--sjdbOverhang 100 '

            # NOTE: for small genomes, may need to scale this down to
            # min(14, log2(GenomeLength) / 2 - 1)
            # --genomeSAindexNbases 14
            '&> {log}'
        )
        # STAR writes a hard-coded Log.out file to the current working
        # directory. So put that on the end of the log file for the rule and
        # then clean up.
        shell('cat {genomedir}/Log.out >> {log} && rm {genomedir}/Log.out')


rule hisat2_index:
    """
    Build HISAT2 index
    """
    input:
        '{references_dir}/{organism}/{tag}/genome/{organism}_{tag}.fasta'
    output:
        protected(aligners.hisat2_index_from_prefix('{references_dir}/{organism}/{tag}/genome/hisat2/{organism}_{tag}'))
    log:
        '{references_dir}/logs/{organism}/{tag}/genome/hisat2/{organism}_{tag}.log'
    run:
        prefix = aligners.prefix_from_hisat2_index(output)
        shell(
            'hisat2-build '
            '{input} '
            '{prefix} '
            '&> {log}')


rule symlink_fasta_to_index_dir:
    """Aligners often want the reference fasta in the same dir as the index, so
    this makes the appropriate symlink
    """
    input:
        fasta='{references_dir}/{organism}/{tag}/{_type}/{organism}_{tag}.fasta'
    output:
        '{references_dir}/{organism}/{tag}/{_type}/{index}/{organism}_{tag}.fasta'
    log:
        '{references_dir}/logs/{organism}/{tag}/{_type}/{index}/{organism}_{tag}.fasta.log'
    run:
        utils.make_relative_symlink(input[0], output[0])


rule transcriptome_fasta:
    input:
        fasta='{references_dir}/{organism}/{tag}/genome/{organism}_{tag}.fasta',
        gtf='{references_dir}/{organism}/{tag}/annotation/{organism}_{tag}.gtf'
    output:
        protected('{references_dir}/{organism}/{tag}/transcriptome/{organism}_{tag}.fasta')
    shell:
        'gffread {input.gtf} -w {output} -g {input.fasta}'


rule salmon_index:
    "Build salmon index"
    output:
        protected('{references_dir}/{organism}/{tag}/transcriptome/salmon/{organism}_{tag}/versionInfo.json')
    input:
        fasta='{references_dir}/{organism}/{tag}/transcriptome/{organism}_{tag}.fasta'
    log:
        '{references_dir}/logs/{organism}/{tag}/transcriptome/salmon/{organism}_{tag}.log'
    params:
        outdir='{references_dir}/{organism}/{tag}/transcriptome/salmon/{organism}_{tag}'
    shell:
        'salmon index '
        '--transcripts {input.fasta} '
        '--index {params.outdir} '
        '&> {log}'


rule conversion_refflat:
    """Converts a GTF into refFlat format
    """
    input:
        '{references_dir}/{organism}/{tag}/annotation/{organism}_{tag}.gtf'
    output:
        protected('{references_dir}/{organism}/{tag}/annotation/{organism}_{tag}.refflat')
    log:
        '{references_dir}/logs/{organism}/{tag}/annotation/{organism}_{tag}.refflat.log'
    shell:
        'gtfToGenePred -ignoreGroupsWithoutExons {input} {output}.tmp '
        '''&& awk '{{print $1"\t"$0}}' {output}.tmp > {output} '''
        '&& rm {output}.tmp '


rule conversion_bed12:
    input:
        '{references_dir}/{organism}/{tag}/annotation/{organism}_{tag}.gtf'
    output:
        protected('{references_dir}/{organism}/{tag}/annotation/{organism}_{tag}.bed12')
    shell:
        'gtfToGenePred -ignoreGroupsWithoutExons {input} {output}.tmp '
        '&& genePredToBed {output}.tmp {output} '
        '&& rm {output}.tmp'

rule conversion_gffutils:
    """Converts a GTF into a gffutils sqlite3 database
    """
    input:
        gtf='{references_dir}/{organism}/{tag}/annotation/{organism}_{tag}.gtf'
    output:
        db=protected('{references_dir}/{organism}/{tag}/annotation/{organism}_{tag}.gtf.db')
    log:
        '{references_dir}/logs/{organism}/{tag}/annotation/{organism}_{tag}.gtf.db.log'
    run:
        import gffutils
        kwargs = conversion_kwargs[output[0]]
        fd, tmpdb = tempfile.mkstemp(suffix='.db', prefix='gffutils_')
        db = gffutils.create_db(data=input.gtf, dbfn=tmpdb, **kwargs)
        shell('mv {tmpdb} {output.db}')


rule chromsizes:
    """Creates a chromsizes table from fasta
    """
    input:
        '{references_dir}/{organism}/{tag}/genome/{organism}_{tag}.fasta'
    output:
        protected('{references_dir}/{organism}/{tag}/genome/{organism}_{tag}.chromsizes')
    log:
        '{references_dir}/logs/{organism}/{tag}/genome/{organism}_{tag}.fasta.log'
    params:
        # NOTE: Be careful with the memory here; make sure you have enough
        # and/or it matches the resources you're requesting in the cluster
        # config.
        java_args='-Xmx20g'
        # java_args='-Xmx2g'  # [TEST SETTINGS -1]
    shell:
        'export LC_COLLATE=C; '
        'rm -f {output}.tmp '
        '&& picard '
        '{params.java_args} '
        'CreateSequenceDictionary R={input} O={output}.tmp &> {log} '
        '&& grep "^@SQ" {output}.tmp '
        '''| awk '{{print $2, $3}}' '''
        '| sed "s/SN://g;s/ LN:/\\t/g" '
        '| sort -k1,1 > {output} '
        '&& rm -f {output}.tmp '


rule genelist:
    """Creates a list of unique gene names in the GTF
    """
    input:
        gtf='{references_dir}/{organism}/{tag}/annotation/{organism}_{tag}.gtf'
    output:
        protected('{references_dir}/{organism}/{tag}/annotation/{organism}_{tag}.genelist')
    run:
        attribute = conversion_kwargs[output[0]]['gene_id']
        import gffutils
        genes = set()
        for feature in gffutils.DataIterator(input.gtf):
            genes.update(feature.attributes[attribute])
        with open(output[0], 'w') as fout:
            for feature in sorted(list(set(genes))):
                fout.write(feature + '\n')


rule mappings:
    """
    Creates gzipped TSV mapping between attributes in the GTF.
    """
    input:
        gtf='{references_dir}/{organism}/{tag}/annotation/{organism}_{tag}.gtf'
    output:
        protected('{references_dir}/{organism}/{tag}/annotation/{organism}_{tag}.mapping.tsv.gz')
    params:
        include_featuretypes=lambda wildcards, output: conversion_kwargs[output[0]].get('include_featuretypes', [])
    run:
        import gffutils

        # Will want to change the setting back to what it was originally when
        # we're done
        orig_setting = gffutils.constants.always_return_list
        gffutils.constants.always_return_list = False

        include_featuretypes = params.include_featuretypes

        res = []
        for f in gffutils.DataIterator(input[0]):

            ft = f.featuretype

            if include_featuretypes and (ft not in include_featuretypes):
                continue

            d = dict(f.attributes)
            d['__featuretype__'] = ft
            res.append(d)

        df = pandas.DataFrame(res)

        # Depending on how many attributes there were and the
        # include_featuretypes settings, this may take a while.
        df = df.drop_duplicates()

        df.to_csv(output[0], sep='\t', index=False, compression='gzip')

        # Restore original setting
        gffutils.constants.always_return_list = orig_setting

# vim: ft=python
