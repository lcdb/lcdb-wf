import os
import sys
sys.path.insert(0, srcdir('../..'))
import gzip
import yaml
import importlib
import tempfile
from tempfile import TemporaryDirectory
import pandas as pd
from snakemake.utils import makedirs
from lib.imports import resolve_name
from lib import utils
from lib.utils import autobump, gb, hours
from lib import aligners, helpers
from lib import common

# Note: when running this workflow on its own (say, to generate all references
# ahead of time) you wil need to provide a config file from the command line.
#
# Otherwise, this file is expected to be `include:`ed into other workflows,
# which will have their own config files.

config = common.load_config(config)

references_dir = common.get_references_dir(config)
refdict, conversion_kwargs = common.references_dict(config)

makedirs([references_dir, os.path.join(references_dir, 'logs')])

localrules: symlink_fasta_to_index_dir

wildcard_constraints:
    _type="genome|transcriptome|annotation",
    _ext="fasta|gtf"


rule all_references:
    input: utils.flatten(refdict)


rule download_and_process:
    """Downloads the configured URL, applies any configured post-processing, and
    saves the resulting gzipped file to *.fasta.gz or *.gtf.gz.
    """
    output:
        temporary('{references_dir}/{organism}/{tag}/{_type}/{organism}_{tag}.{_ext}.gz')
    run:
        common.download_and_postprocess(output[0], config, wildcards.organism, wildcards.tag, wildcards._type)


rule unzip:
    """Generic rule to unzip files as needed, for example when building
    indexes.
    """
    input:
        rules.download_and_process.output
    output:
        protected('{references_dir}/{organism}/{tag}/{_type}/{organism}_{tag}.{_ext}')
    wildcard_constraints:
        _type="genome|annotation"
    log:
        '{references_dir}/logs/{organism}/{tag}/{_type}/{organism}_{tag}.{_ext}.log'
    shell: 'gunzip -c {input} > {output}'

rule bwa_index:
    """
    Build bwa index
    """
    input:
        '{references_dir}/{organism}/{tag}/genome/{organism}_{tag}.fasta'
    output:
        protected(aligners.bwa_index_from_prefix('{references_dir}/{organism}/{tag}/genome/bwa/{organism}_{tag}'))
    log:
        '{references_dir}/logs/{organism}/{tag}/genome/bwa/{organism}_{tag}.log'
    resources:
        runtime=autobump(hours=3),
        mem_mb=gb(24),
        disk_mb=gb(24)
    run:
        prefix=aligners.bwa_prefix_from_index(output)
        print(prefix)
        shell(
            'bwa index '
            '-p {prefix} '
            '-a bwtsw '
            '{input} '
            '&> {log}')

rule bowtie2_index:
    """
    Build bowtie2 index
    """
    input:
        '{references_dir}/{organism}/{tag}/genome/{organism}_{tag}.fasta'
    output:
        protected(aligners.bowtie2_index_from_prefix('{references_dir}/{organism}/{tag}/genome/bowtie2/{organism}_{tag}'))
    log:
        '{references_dir}/logs/{organism}/{tag}/genome/bowtie2/{organism}_{tag}.log'
    resources:
        runtime=autobump(hours=8),
        mem_mb=autobump(gb=32),
        disk_mb=autobump(gb=50)
    run:
        prefix = aligners.prefix_from_bowtie2_index(output)
        shell(
            'bowtie2-build '
            '{input} '
            '{prefix} '
            '&> {log}')


rule star_index:
    input:
        fasta='{references_dir}/{organism}/{tag}/genome/{organism}_{tag}.fasta',
        gtf='{references_dir}/{organism}/{tag}/annotation/{organism}_{tag}.gtf',
    output:
        protected('{references_dir}/{organism}/{tag}/genome/star/{organism}_{tag}/Genome')
    log:
        '{references_dir}/{organism}/{tag}/genome/star/{organism}_{tag}/Genome.log'
    threads:
        8
    resources:
        runtime=autobump(hours=8),
        mem_mb=gb(64)
    run:
        genomedir = os.path.dirname(output[0])
        shell('rm -r {genomedir}')
        shell('mkdir -p {genomedir}')
        shell(
            'STAR '
            '--runMode genomeGenerate '
            '--runThreadN {threads} '
            '--genomeDir {genomedir} '
            '--genomeFastaFiles {input.fasta} '

            # NOTE: GTF is optional
            '--sjdbGTFfile {input.gtf} '

            # NOTE: STAR docs say that 100 should work well.
            '--sjdbOverhang 100 '

            # NOTE: for small genomes, may need to scale this down to
            # min(14, log2(GenomeLength) / 2 - 1)
            # --genomeSAindexNbases 14
            '&> {log}'
        )
        # STAR writes a hard-coded Log.out file to the current working
        # directory. So put that on the end of the log file for the rule and
        # then clean up.
        shell('cat {genomedir}/Log.out >> {log} && rm {genomedir}/Log.out')


rule hisat2_index:
    """
    Build HISAT2 index
    """
    input:
        '{references_dir}/{organism}/{tag}/genome/{organism}_{tag}.fasta'
    output:
        protected(aligners.hisat2_index_from_prefix('{references_dir}/{organism}/{tag}/genome/hisat2/{organism}_{tag}'))
    log:
        '{references_dir}/logs/{organism}/{tag}/genome/hisat2/{organism}_{tag}.log'
    resources:
        runtime=autobump(hours=8),
        mem_mb=gb(32),
        disk_mb=gb(50)
    run:
        prefix = aligners.prefix_from_hisat2_index(output)
        shell(
            'hisat2-build '
            '{input} '
            '{prefix} '
            '&> {log}')


rule symlink_fasta_to_index_dir:
    """Aligners often want the reference fasta in the same dir as the index, so
    this makes the appropriate symlink
    """
    input:
        fasta='{references_dir}/{organism}/{tag}/{_type}/{organism}_{tag}.fasta'
    output:
        '{references_dir}/{organism}/{tag}/{_type}/{index}/{organism}_{tag}.fasta'
    resources:
        runtime=hours(1)
    log:
        '{references_dir}/logs/{organism}/{tag}/{_type}/{index}/{organism}_{tag}.fasta.log'
    run:
        utils.make_relative_symlink(input[0], output[0])


rule transcriptome_fasta:
    input:
        fasta='{references_dir}/{organism}/{tag}/genome/{organism}_{tag}.fasta',
        gtf='{references_dir}/{organism}/{tag}/annotation/{organism}_{tag}.gtf'
    output:
        protected('{references_dir}/{organism}/{tag}/transcriptome/{organism}_{tag}.fasta')
    resources:
        runtime=hours(1)
    shell:
        'gffread {input.gtf} -w {output} -g {input.fasta}'


rule salmon_index:
    "Build salmon index"
    output:
        protected('{references_dir}/{organism}/{tag}/transcriptome/salmon/{organism}_{tag}/versionInfo.json')
    input:
        fasta='{references_dir}/{organism}/{tag}/transcriptome/{organism}_{tag}.fasta'
    log:
        '{references_dir}/logs/{organism}/{tag}/transcriptome/salmon/{organism}_{tag}.log'
    params:
        outdir='{references_dir}/{organism}/{tag}/transcriptome/salmon/{organism}_{tag}'
    resources:
        mem_mb=gb(32),
        runtime=hours(2)
    shell:
        'salmon index '
        '--transcripts {input.fasta} '
        '--index {params.outdir} '
        '&> {log}'


rule kallisto_index:
    "Build kallisto index"
    output:
        index=protected('{references_dir}/{organism}/{tag}/transcriptome/kallisto/{organism}_{tag}/transcripts.idx')
    input:
        fasta='{references_dir}/{organism}/{tag}/transcriptome/{organism}_{tag}.fasta'
    log:
        '{references_dir}/logs/{organism}/{tag}/transcriptome/kallisto/{organism}_{tag}.log'
    resources:
        runtime=hours(2),
        mem_mb=gb(32),
    shell:
        'kallisto index '
        '--index {output.index} '
        '{input.fasta} '
        '&> {log}'


rule conversion_refflat:
    """Converts a GTF into refFlat format
    """
    input:
        '{references_dir}/{organism}/{tag}/annotation/{organism}_{tag}.gtf'
    output:
        protected('{references_dir}/{organism}/{tag}/annotation/{organism}_{tag}.refflat')
    log:
        '{references_dir}/logs/{organism}/{tag}/annotation/{organism}_{tag}.refflat.log'
    resources:
        runtime=hours(2),
        mem_mb=gb(2)
    shell:
        'gtfToGenePred -ignoreGroupsWithoutExons {input} {output}.tmp '
        '''&& awk '{{print $1"\t"$0}}' {output}.tmp > {output} '''
        '&& rm {output}.tmp '


rule conversion_bed12:
    input:
        '{references_dir}/{organism}/{tag}/annotation/{organism}_{tag}.gtf'
    output:
        protected('{references_dir}/{organism}/{tag}/annotation/{organism}_{tag}.bed12')
    resources:
        runtime=hours(2),
        mem_mb=gb(2)
    shell:
        'gtfToGenePred -ignoreGroupsWithoutExons {input} {output}.tmp '
        '&& genePredToBed {output}.tmp {output} '
        '&& rm {output}.tmp'

rule conversion_gffutils:
    """Converts a GTF into a gffutils sqlite3 database
    """
    input:
        gtf='{references_dir}/{organism}/{tag}/annotation/{organism}_{tag}.gtf'
    output:
        db=protected('{references_dir}/{organism}/{tag}/annotation/{organism}_{tag}.gtf.db')
    log:
        '{references_dir}/logs/{organism}/{tag}/annotation/{organism}_{tag}.gtf.db.log'
    resources:
        runtime=hours(2),
        mem_mb=gb(4)
    run:
        import gffutils
        kwargs = conversion_kwargs[output[0]]
        fd, tmpdb = tempfile.mkstemp(suffix='.db', prefix='gffutils_')
        db = gffutils.create_db(data=input.gtf, dbfn=tmpdb, **kwargs)
        shell('mv {tmpdb} {output.db}')


rule chromsizes:
    """Creates a chromsizes table from fasta
    """
    input:
        '{references_dir}/{organism}/{tag}/genome/{organism}_{tag}.fasta'
    output:
        protected('{references_dir}/{organism}/{tag}/genome/{organism}_{tag}.chromsizes')
    log:
        '{references_dir}/logs/{organism}/{tag}/genome/{organism}_{tag}.fasta.log'
    params:
        # NOTE: Be careful with the memory here; make sure you have enough
        # and/or it matches the resources you're requesting in the cluster
        # config.
        java_args='-Xmx20g'
        # java_args='-Xmx2g'  # [TEST SETTINGS -1]
    resources:
        mem_mb=gb(24),
        runtime=hours(2)
    shell:
        'export LC_COLLATE=C; '
        'rm -f {output}.tmp '
        '&& picard '
        '{params.java_args} '
        'CreateSequenceDictionary R={input} O={output}.tmp &> {log} '
        '&& grep "^@SQ" {output}.tmp '
        '''| awk '{{print $2, $3}}' '''
        '| sed "s/SN://g;s/ LN:/\\t/g" '
        '| sort -k1,1 > {output} '
        '&& rm -f {output}.tmp '


rule genelist:
    """Creates a list of unique gene names in the GTF
    """
    input:
        gtf='{references_dir}/{organism}/{tag}/annotation/{organism}_{tag}.gtf'
    output:
        protected('{references_dir}/{organism}/{tag}/annotation/{organism}_{tag}.genelist')
    resources:
        runtime=hours(1),
        mem_mb=gb(2)
    run:
        attribute = conversion_kwargs[output[0]]['gene_id']
        import gffutils
        genes = set()
        for feature in gffutils.DataIterator(input.gtf):
            genes.update(feature.attributes[attribute])
        with open(output[0], 'w') as fout:
            for feature in sorted(list(set(genes))):
                fout.write(feature + '\n')


rule mappings:
    """
    Creates gzipped TSV mapping between attributes in the GTF.
    """
    input:
        gtf='{references_dir}/{organism}/{tag}/annotation/{organism}_{tag}.gtf'
    output:
        protected('{references_dir}/{organism}/{tag}/annotation/{organism}_{tag}.mapping.tsv.gz')
    params:
        include_featuretypes=lambda wildcards, output: conversion_kwargs[output[0]].get('include_featuretypes', [])
    resources:
        runtime=hours(2),
        mem_mb=gb(2)
    run:
        import gffutils

        # Will want to change the setting back to what it was originally when
        # we're done
        orig_setting = gffutils.constants.always_return_list
        gffutils.constants.always_return_list = False

        include_featuretypes = params.include_featuretypes

        res = []
        for f in gffutils.DataIterator(input[0]):

            ft = f.featuretype

            if include_featuretypes and (ft not in include_featuretypes):
                continue

            d = dict(f.attributes)
            d['__featuretype__'] = ft
            res.append(d)

        df = pd.DataFrame(res)

        # Depending on how many attributes there were and the
        # include_featuretypes settings, this may take a while.
        df = df.drop_duplicates()

        df.to_csv(output[0], sep='\t', index=False, compression='gzip')

        # Restore original setting
        gffutils.constants.always_return_list = orig_setting


checkpoint genome_index:
    """
    Build fasta index. GATK uses this file for rapid fasta accession as well as
    the fasta index is used to identify chromosomes and contigs in the genome
    to download the appropriate known variation file
    """
    input:
        '{references_dir}/{organism}/{tag}/genome/{organism}_{tag}.fasta'
    output:
        protected('{references_dir}/{organism}/{tag}/genome/faidx/{organism}_{tag}.fai')
    log:
        '{references_dir}/logs/{organism}/{tag}/genome/faidx/{organism}_{tag}.fai.log'
    resources:
        runtime=hours(1),
        mem_mb=gb(4)
    run:
        shell(
            'samtools '
            'faidx '
            '-o {output} {input} '
            '&> {log}'
        )


rule known_variation:
    """
    Download all the chromosomes on the ensembl ftp site and combine them to
    create a known variation vcf
    """
    input:
        #fai='{references_dir}/{organism}/{tag}/genome/faidx/{organism}_{tag}.fai'
        # can't do it this way since the {tag} wildcard is not congruous between input and output
        fai = lambda w: checkpoints.genome_index.get(**w).output[0]

    output:
        protected('{references_dir}/{organism}/{tag}/known/{organism}_{tag}.vcf.gz')
    log:
        '{references_dir}/{organism}/{tag}/known/{organism}_{tag}.known.log'
    resources:
        runtime=hours(4),
        mem_mb=gb(16),
        disk_mb=gb(32)
    run:
        # Get the configuration options in the metadata chunk using wildcards
        release = int(config['references'][wildcards.organism][wildcards.tag]['metadata']['release'])
        species = config['references'][wildcards.organism][wildcards.tag]['metadata']['species']
        build = config['references'][wildcards.organism][wildcards.tag]['metadata']['build']
        typ = config['references'][wildcards.organism][wildcards.tag]['known']['type']
        def get_contigs():
            with open(input[0], 'r') as fai:
                ser =  pd.read_table(fai, header=None, usecols=[0], dtype=str)
                ser = ser.squeeze()
                ser =  ser[ser.apply(lambda x: len(x)) <= 2]
            return ser
        contigs = get_contigs()
        branch=""
        if release >= 81 and build == "GRCh37":
            branch="grch37/"
        if typ == "all":
            if species == "homo_sapiens" and release >= 93:
                suffixes = [
            "-chr{}".format(chrom) for chrom in contigs
        ]
            else:
                suffixes = [""]
        elif typ == "somatic":
            suffixes=["_somatic"]
        elif typ == "structural_variations":
            suffixes=["_structural_variations"]
        species = species.lower()
        release = int(release)
        build = build
        typ = typ
        species_filename=species if release >= 91 else species.capitalize()
        urls=[
            "ftp://ftp.ensembl.org/pub/{branch}release-{release}/variation/vcf/{species}/{species_filename}{suffix}.{ext}".format(
                release=release,
                species=species,
                suffix=suffix,
                species_filename=species_filename,
                branch=branch,
                ext=ext,
            )
            for suffix in suffixes
            for ext in ["vcf.gz", "vcf.gz.csi"]
        ]
        names=[os.path.basename(url) for url in urls if url.endswith(".gz")]
        gather = "curl {urls}".format(urls=" ".join(map("-O {}".format, urls)))
        with tempfile.TemporaryDirectory() as tmpdir:
            if input.get("fai"):
                shell(
                    "(cd {tmpdir}; {gather} && "
                    "bcftools concat -Oz --naive-force {names} > concat.vcf.gz && "
                    "bcftools reheader --fai {input.fai} concat.vcf.gz "
                    "> {output}) && "
                    "tabix -p vcf {output} 2> {log} "
                )

#if config['references']['human'].get('variation'):
#    rule dbnsfp:
#        """
#        Download and process dbNSFP database. This involves downloading and
#        extracting the zip file, then combining the chromosomes to create
#        a single file. For genome builds like hg19 and GRCh37, some processing
#        needs to be done to make them compatible with dbNSFP version > 3.X
#        dbNSFP is only for human genomes.
#        """
#        output:
#            protected(
#                '{{references_dir}}/{{organism}}/{{tag}}/{dbnsfp_version}_{build}/{{organism}}_{{tag}}.vcf.gz'.format(
#                    dbnsfp_version=config['references']['human']['variation']['dbnsfp']['version'],
#                    build=config['references']['human']['variation']['dbnsfp']['build']
#                )
#            )
#        log:
#            '{{references_dir}}/{{organism}}/{{tag}}/{dbnsfp_version}_{build}/{{organism}}_{{tag}}.log'.format(
#                dbnsfp_version=config['references']['human']['variation']['dbnsfp']['version'],
#                build=config['references']['human']['variation']['dbnsfp']['build']
#            )
#        resources:
#            disk_mb=gb(500),
#            mem_mb=gb(500),
#            runtime=hours(8)
#        threads: 16
#        run:
#            version = config['references']['human'][wildcards.tag]['dbnsfp']['version']
#            URL = config['references']['human'][wildcards.tag]['dbnsfp']['url']
#            build = config['references']['human'][wildcards.tag]['dbnsfp']['build']
#            workdir = wildcards.references_dir
#            if build == 'GRCh37':
#                # We need to process the dbNSFP file to make it compatible with older genomes
#                with tempfile.TemporaryDirectory() as tmpdir:
#                    shell(
#                        '''(cd {tmpdir}; wget -O- {URL} > dbnsfp.zip && '''
#                        '''unzip dbnsfp.zip && zcat dbNSFP*_variant.chr1* | awk "NR<=1" > h && '''
#                        '''zgrep -v "^#" dbNSFP*_variant.chr* > all_chrs && '''
#                        '''awk '$8 != "." ' all_chrs > all_chrs_filtered && '''
#                        '''sort -S 50% --parallel=12 all_chrs_filtered -k8,8 -k9,9n > all_chrs_filtered_sorted && '''
#                        '''cat h all_chrs_filtered_sorted > all_chrs_filtered_sorted_header && '''
#                        '''bgzip -c all_chrs_filtered_sorted_header > {output}) && '''
#                        '''tabix -s 8 -b 9 -e 9 {output} '''
#                    )
#            if build == 'GRCh38':
#                with tempfile.TemporaryDirectory() as tmpdir:
#                    # No need for processing and we can use the first 2 columns for coordinates
#                    shell(
#                        '''(cd {tmpdir}; wget -O- {URL} > dbnsfp.zip && '''
#                        '''unzip dbnsfp.zip && zcat dbNSFP*_variant.chr1* | awk "NR<=1" > h && '''
#                        '''zgrep -v "^#" dbNSFP*_variant.chr* > all_chrs && '''
#                        '''sort -S 50% --parallel=24 all_chrs -k1,1 -k2,2n > all_chrs_sorted && '''
#                        '''cat h all_chrs_sorted > all_chrs_sorted_header && '''
#                        '''bgzip -c all_chrs_sorted_header > {output}) && '''
#                        '''tabix -s 1 -b 2 -e 2 {output} '''
#                    )
#

# vim: ft=python
